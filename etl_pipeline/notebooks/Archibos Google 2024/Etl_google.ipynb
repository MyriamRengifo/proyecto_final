{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Después de descargar el archivo desde la pagina provista de Henry (https://drive.google.com/drive/folders/1Wf7YkxA0aHI3GpoHc9Nh8_scf5BbD4DA) paralos archivos de google  procedemos  a realisar el etl. Los datos del archivo descargado se encuentran en varios archivos por ello haremos varios merge hasta unificar los archivos en uno solo"
      ],
      "metadata": {
        "id": "Db1BC6-8vpwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLVpHS1rGk51"
      },
      "outputs": [],
      "source": [
        "#Librerias\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import ast\n",
        "import random\n",
        "import string\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gargamos el archivo reviews corespondientes al estado de florida en .json y los pasamos a .csv"
      ],
      "metadata": {
        "id": "m80bPvwtINg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los directorios a cargar y guardar\n",
        "source_dir = '/content/drive/MyDrive/data/datos en bruto/reviews-estados/review-Florida'\n",
        "target_dir = '/content/sample_data/revius gogle'\n",
        "\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Listamos los  archivos en el directorio fuente\n",
        "files = [f for f in os.listdir(source_dir) if f.endswith(('.json'))]\n",
        "\n",
        "# Procesamos lo archivos y los convertimos a .csv\n",
        "converted_files = []\n",
        "\n",
        "for file in files:\n",
        "    source_path = os.path.join(source_dir, file)\n",
        "    target_path = os.path.join(target_dir, os.path.splitext(file)[0] + '.csv')\n",
        "\n",
        "    try:\n",
        "        if file.endswith('.csv'):\n",
        "            df = pd.read_csv(source_path)\n",
        "        elif file.endswith('.txt'):\n",
        "            df = pd.read_csv(source_path, sep='\\t', header=None)\n",
        "        elif file.endswith('.json'):\n",
        "            with open(source_path, 'r', encoding='utf-8') as f:\n",
        "                data = [json.loads(line) for line in f]\n",
        "            df = pd.DataFrame(data)\n",
        "        elif file.endswith('.xlsx'):\n",
        "            df = pd.read_excel(source_path)\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Guardamos como CSV con carácter de escape\n",
        "        df.to_csv(target_path, index=False, escapechar='\\\\')\n",
        "        converted_files.append(target_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file}: {e}\")\n",
        "\n",
        "# Mostramos archivos convertidos\n",
        "print(\"Archivos convertidos a CSV:\")\n",
        "for file in converted_files:\n",
        "    print(file)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ll5xDzdJPbP",
        "outputId": "db4dbae9-bcd5-4af7-c207-db6248a63778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos convertidos a CSV:\n",
            "/content/sample_data/revius gogle/1.csv\n",
            "/content/sample_data/revius gogle/2.csv\n",
            "/content/sample_data/revius gogle/3.csv\n",
            "/content/sample_data/revius gogle/4.csv\n",
            "/content/sample_data/revius gogle/5.csv\n",
            "/content/sample_data/revius gogle/6.csv\n",
            "/content/sample_data/revius gogle/7.csv\n",
            "/content/sample_data/revius gogle/8.csv\n",
            "/content/sample_data/revius gogle/9.csv\n",
            "/content/sample_data/revius gogle/10.csv\n",
            "/content/sample_data/revius gogle/11.csv\n",
            "/content/sample_data/revius gogle/12.csv\n",
            "/content/sample_data/revius gogle/13.csv\n",
            "/content/sample_data/revius gogle/14.csv\n",
            "/content/sample_data/revius gogle/15.csv\n",
            "/content/sample_data/revius gogle/16.csv\n",
            "/content/sample_data/revius gogle/17.csv\n",
            "/content/sample_data/revius gogle/18.csv\n",
            "/content/sample_data/revius gogle/19.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cargmos las rutas de los archivos revius gogle que se van a combinar\n",
        "files_to_merge = [\n",
        "    '/content/sample_data/revius gogle/1.csv',\n",
        "    '/content/sample_data/revius gogle/2.csv',\n",
        "    '/content/sample_data/revius gogle/3.csv',\n",
        "    '/content/sample_data/revius gogle/4.csv',\n",
        "    '/content/sample_data/revius gogle/5.csv',\n",
        "    '/content/sample_data/revius gogle/6.csv',\n",
        "    '/content/sample_data/revius gogle/7.csv',\n",
        "    '/content/sample_data/revius gogle/8.csv',\n",
        "    '/content/sample_data/revius gogle/9.csv',\n",
        "    '/content/sample_data/revius gogle/10.csv',\n",
        "    '/content/sample_data/revius gogle/11.csv',\n",
        "    '/content/sample_data/revius gogle/12.csv',\n",
        "    '/content/sample_data/revius gogle/13.csv',\n",
        "    '/content/sample_data/revius gogle/14.csv',\n",
        "    '/content/sample_data/revius gogle/15.csv',\n",
        "    '/content/sample_data/revius gogle/16.csv',\n",
        "    '/content/sample_data/revius gogle/17.csv',\n",
        "    '/content/sample_data/revius gogle/18.csv',\n",
        "    '/content/sample_data/revius gogle/19.csv'\n",
        "\n",
        "]\n",
        "\n",
        "# Leemos y combinamos los archivos\n",
        "dataframes = []\n",
        "for file in files_to_merge:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error leyendo {file}: {e}\")\n",
        "\n",
        "# Realizamos el merge corespondiente\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardamos el archivo combinado con el nombre reviews1.csv\n",
        "output_path = '/content/sample_data/revius_Florida.csv'\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_65zins9WfNW",
        "outputId": "850e43f0-a647-4e2b-c3a1-39c8e1253e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sample_data/revius_Florida.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos lo directorios de origen y destino\n",
        "source_dir = '/content/drive/MyDrive/data/datos en bruto/metadata-sitios'\n",
        "target_dir = '/content/sample_data/metadataFlorida'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "json_files = [f for f in os.listdir(source_dir) if f.endswith('.json')]\n",
        "\n",
        "# Convertimos cada archivo .json a .csv\n",
        "converted_files = []\n",
        "\n",
        "for file in json_files:\n",
        "    source_path = os.path.join(source_dir, file)\n",
        "    target_path = os.path.join(target_dir, os.path.splitext(file)[0] + '.csv')\n",
        "\n",
        "    try:\n",
        "        with open(source_path, 'r', encoding='utf-8') as f:\n",
        "            data = [json.loads(line) for line in f]\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Guardamos el archivo resultante como .csv\n",
        "        df.to_csv(target_path, index=False)\n",
        "        converted_files.append(target_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {file}: {e}\")\n",
        "\n",
        "converted_files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoZPaSE0ZkJ5",
        "outputId": "463d6145-2052-4c75-85c5-0171f3aea523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/sample_data/metadataFlorida/1.csv',\n",
              " '/content/sample_data/metadataFlorida/2.csv',\n",
              " '/content/sample_data/metadataFlorida/3.csv',\n",
              " '/content/sample_data/metadataFlorida/4.csv',\n",
              " '/content/sample_data/metadataFlorida/5.csv',\n",
              " '/content/sample_data/metadataFlorida/6.csv',\n",
              " '/content/sample_data/metadataFlorida/7.csv',\n",
              " '/content/sample_data/metadataFlorida/8.csv',\n",
              " '/content/sample_data/metadataFlorida/9.csv',\n",
              " '/content/sample_data/metadataFlorida/10.csv',\n",
              " '/content/sample_data/metadataFlorida/11.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las rutas de los archivos que usaremos para el merge\n",
        "files_to_merge = [\n",
        "    '/content/sample_data/metadataFlorida/8.csv',\n",
        "    '/content/sample_data/metadataFlorida/10.csv',\n",
        "    '/content/sample_data/metadataFlorida/11.csv'\n",
        "]\n",
        "\n",
        "# Leemos y combinamos los archivos\n",
        "dataframes = []\n",
        "for file in files_to_merge:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error leyendo {file}: {e}\")\n",
        "\n",
        "# Realizamos el merge correspondiente\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardar el archivo combinado como reviews1.csv\n",
        "output_path = '/content/sample_data/metada1'\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KPSKMps9Z8ch",
        "outputId": "e2538cde-8987-4121-a4d7-2b72ec6aa915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sample_data/metada1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las rutas de los archivos que vamos a usar para el merge\n",
        "files_to_merge = [\n",
        "    '/content/sample_data/metadataFlorida/1.csv',\n",
        "    '/content/sample_data/metadataFlorida/2.csv',\n",
        "    '/content/sample_data/metadataFlorida/3.csv',\n",
        "    '/content/sample_data/metadataFlorida/4.csv',\n",
        "    '/content/sample_data/metadataFlorida/5.csv',\n",
        "    '/content/sample_data/metadataFlorida/6.csv',\n",
        "    '/content/sample_data/metadataFlorida/7.csv'\n",
        "\n",
        "]\n",
        "\n",
        "# Leemos y combinamos  los archivos\n",
        "dataframes = []\n",
        "for file in files_to_merge:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        dataframes.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Error leyendo {file}: {e}\")\n",
        "\n",
        "# Realizamos el merge correspondiente\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Guardamos el archivo combinado como reviews1.csv\n",
        "output_path = '/content/sample_data/metada2 '\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "\n",
        "output_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KLl1ZDv0Z819",
        "outputId": "112fff01-a802-498f-92e7-4c73d706c1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sample_data/metada2 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos desde los archivos.csv\n",
        "def cargar_datos_csv(filepath):\n",
        "    return pd.read_csv(filepath)\n",
        "\n",
        "# Archivos de entrada\n",
        "archivo_reseñas = \"/content/sample_data/revius_Florida.csv\"\n",
        "archivo_metadata1 = \"/content/sample_data/metada1\"\n",
        "archivo_metadata2 = \"/content/sample_data/metada2 \"\n",
        "\n",
        "reseñas = cargar_datos_csv(archivo_reseñas)\n",
        "metadata1 = cargar_datos_csv(archivo_metadata1)\n",
        "metadata2 = cargar_datos_csv(archivo_metadata2)\n",
        "\n",
        "# Combinamos los archivos metadata\n",
        "metadata = pd.concat([metadata1, metadata2], ignore_index=True)\n",
        "\n",
        "# Realizamos un join usando 'gmap_id' como clave\n",
        "result = reseñas.merge(metadata, on=\"gmap_id\", how=\"inner\")\n",
        "\n",
        "# Mostramos los resultados\n",
        "print(\"Resumen del DataFrame combinado:\")\n",
        "print(result.info())\n",
        "print(\"\\nPrimeras filas del DataFrame combinado:\")\n",
        "print(result.head())\n",
        "\n",
        "# Guardamos el resultado en un archivo .csv\n",
        "output_path = \"/content/sample_data/gogle1.csv\"\n",
        "result.to_csv(output_path, index=False)\n",
        "print(f\"Datos combinados guardados en: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x2M394ncsXf",
        "outputId": "964719c5-1877-4005-ccd6-1ceb7aeeb66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen del DataFrame combinado:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2869646 entries, 0 to 2869645\n",
            "Data columns (total 22 columns):\n",
            " #   Column            Dtype  \n",
            "---  ------            -----  \n",
            " 0   user_id           object \n",
            " 1   name_x            object \n",
            " 2   time              int64  \n",
            " 3   rating            int64  \n",
            " 4   text              object \n",
            " 5   pics              object \n",
            " 6   resp              object \n",
            " 7   gmap_id           object \n",
            " 8   name_y            object \n",
            " 9   address           object \n",
            " 10  description       object \n",
            " 11  latitude          float64\n",
            " 12  longitude         float64\n",
            " 13  category          object \n",
            " 14  avg_rating        float64\n",
            " 15  num_of_reviews    int64  \n",
            " 16  price             object \n",
            " 17  hours             object \n",
            " 18  MISC              object \n",
            " 19  state             object \n",
            " 20  relative_results  object \n",
            " 21  url               object \n",
            "dtypes: float64(3), int64(3), object(16)\n",
            "memory usage: 481.7+ MB\n",
            "None\n",
            "\n",
            "Primeras filas del DataFrame combinado:\n",
            "                 user_id            name_x           time  rating  \\\n",
            "0  101471856155148729010   Julie A. Gerber  1628003250740       1   \n",
            "1  101471856155148729010   Julie A. Gerber  1628003250740       1   \n",
            "2  115477234789038326051  Martin Sheffield  1595031217005       5   \n",
            "3  115477234789038326051  Martin Sheffield  1595031217005       5   \n",
            "4  101805010244892834381      Brian Truett  1522924253567       5   \n",
            "\n",
            "                                                text pics  \\\n",
            "0  Update: Their “reply” to my review amounted to...  NaN   \n",
            "1  Update: Their “reply” to my review amounted to...  NaN   \n",
            "2  He's a knowledgeable doctor but the way he run...  NaN   \n",
            "3  He's a knowledgeable doctor but the way he run...  NaN   \n",
            "4  Best doctor I've ever had, I never wait to be ...  NaN   \n",
            "\n",
            "                                                resp  \\\n",
            "0  {'time': 1627042799532, 'text': 'Thank you for...   \n",
            "1  {'time': 1627042799532, 'text': 'Thank you for...   \n",
            "2  {'time': 1582464056733, 'text': 'Thank you for...   \n",
            "3  {'time': 1582464056733, 'text': 'Thank you for...   \n",
            "4                                                NaN   \n",
            "\n",
            "                                 gmap_id             name_y  \\\n",
            "0  0x8893863ea87bd5dd:0x9383ebf973e74abb  Brian Shaheen, MD   \n",
            "1  0x8893863ea87bd5dd:0x9383ebf973e74abb  Brian Shaheen, MD   \n",
            "2  0x8893863ea87bd5dd:0x9383ebf973e74abb  Brian Shaheen, MD   \n",
            "3  0x8893863ea87bd5dd:0x9383ebf973e74abb  Brian Shaheen, MD   \n",
            "4  0x8893863ea87bd5dd:0x9383ebf973e74abb  Brian Shaheen, MD   \n",
            "\n",
            "                                             address  ...  longitude  \\\n",
            "0  Brian Shaheen, MD, 2421 Thomas Dr, Panama City...  ... -85.752277   \n",
            "1  Brian Shaheen, MD, 2421 Thomas Dr, Panama City...  ... -85.752277   \n",
            "2  Brian Shaheen, MD, 2421 Thomas Dr, Panama City...  ... -85.752277   \n",
            "3  Brian Shaheen, MD, 2421 Thomas Dr, Panama City...  ... -85.752277   \n",
            "4  Brian Shaheen, MD, 2421 Thomas Dr, Panama City...  ... -85.752277   \n",
            "\n",
            "                                            category  avg_rating  \\\n",
            "0  ['Family practice physician', 'General practit...         4.2   \n",
            "1  ['Family practice physician', 'General practit...         4.2   \n",
            "2  ['Family practice physician', 'General practit...         4.2   \n",
            "3  ['Family practice physician', 'General practit...         4.2   \n",
            "4  ['Family practice physician', 'General practit...         4.2   \n",
            "\n",
            "  num_of_reviews  price                                              hours  \\\n",
            "0             18    NaN  [['Thursday', '8AM–5PM'], ['Friday', '8AM–5PM'...   \n",
            "1             18    NaN  [['Thursday', '8AM–5PM'], ['Friday', '8AM–5PM'...   \n",
            "2             18    NaN  [['Thursday', '8AM–5PM'], ['Friday', '8AM–5PM'...   \n",
            "3             18    NaN  [['Thursday', '8AM–5PM'], ['Friday', '8AM–5PM'...   \n",
            "4             18    NaN  [['Thursday', '8AM–5PM'], ['Friday', '8AM–5PM'...   \n",
            "\n",
            "                                                MISC              state  \\\n",
            "0  {'Service options': ['Online care'], 'Accessib...  Open ⋅ Closes 5PM   \n",
            "1  {'Service options': ['Online care'], 'Accessib...  Open ⋅ Closes 5PM   \n",
            "2  {'Service options': ['Online care'], 'Accessib...  Open ⋅ Closes 5PM   \n",
            "3  {'Service options': ['Online care'], 'Accessib...  Open ⋅ Closes 5PM   \n",
            "4  {'Service options': ['Online care'], 'Accessib...  Open ⋅ Closes 5PM   \n",
            "\n",
            "                                    relative_results  \\\n",
            "0  ['0x88938ebfbd53f9c5:0xf6e52004f37523c8', '0x8...   \n",
            "1  ['0x88938ebfbd53f9c5:0xf6e52004f37523c8', '0x8...   \n",
            "2  ['0x88938ebfbd53f9c5:0xf6e52004f37523c8', '0x8...   \n",
            "3  ['0x88938ebfbd53f9c5:0xf6e52004f37523c8', '0x8...   \n",
            "4  ['0x88938ebfbd53f9c5:0xf6e52004f37523c8', '0x8...   \n",
            "\n",
            "                                                 url  \n",
            "0  https://www.google.com/maps/place//data=!4m2!3...  \n",
            "1  https://www.google.com/maps/place//data=!4m2!3...  \n",
            "2  https://www.google.com/maps/place//data=!4m2!3...  \n",
            "3  https://www.google.com/maps/place//data=!4m2!3...  \n",
            "4  https://www.google.com/maps/place//data=!4m2!3...  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Datos combinados guardados en: /content/sample_data/gogle1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una ves que ya tenemos un archivo combinado con todos los archivos que descargamos anteriormente procedemos a realizar un diccionario de categorías para clasificar los datos según su categoría los datos que no se definan en una categoría particular se guardaran en el archivo otros."
      ],
      "metadata": {
        "id": "sYpJl1dFOW7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos la ruta del archivo despues del merge\n",
        "file_path = '/content/drive/MyDrive/gogle1.csv'\n",
        "\n",
        "# Creamos un diccionario para agrupar categorías específicas en grupos generales\n",
        "category_mapping = {\n",
        "    \"restaurantes\": [\n",
        "        \"restaurant\", \"cafe\", \"espresso bar\", \"dart bar\", \"hookah bar\", \"cafeteria\",\n",
        "        \"cocktail bar\", \"fast food\", \"pizza\", \"grill\", \"barbecue\", \"bistro\", \"brunch\",\n",
        "        \"buffet\", \"bar\"\n",
        "    ],\n",
        "    \"educacion\": [\n",
        "        \"school\", \"college\", \"private educational institution\", \"university\",\n",
        "        \"kindergarten\", \"vocational school\", \"dance school\", \"music school\",\n",
        "        \"education\", \"education center\", \"music conservatory\", \"music store\",\n",
        "        \"music instructor\"\n",
        "    ],\n",
        "    \"supermercados_tiendas_negocios\": [\n",
        "        \"supermarket\", \"grocery store\", \"food store\", \"bakery\", \"candy store\",\n",
        "        \"ice cream shop\", \"convenience store\", \"boutique\", \"furniture store\"\n",
        "    ],\n",
        "    \"entretenimiento\": [\n",
        "        \"cinema\", \"theater\", \"sports complex\", \"aquarium\", \"zoo\", \"museum\",\n",
        "        \"concert\", \"gaming\", \"amusement park\"\n",
        "    ],\n",
        "    \"salud\": [\n",
        "        \"hospital\", \"clinic\", \"dentist\", \"medical center\", \"pharmacy\",\n",
        "        \"treatment center\", \"psychologist\", \"cardiologist\", \"nutritionist\"\n",
        "    ],\n",
        "    \"transporte\": [\n",
        "        \"bus station\", \"train station\", \"airport\", \"taxi\", \"vehicle rental\",\n",
        "        \"transportation service\", \"bicycle shop\", \"car rental\"\n",
        "    ],\n",
        "    \"servicios\": [\n",
        "        \"government office\", \"bank\", \"atm\", \"post office\", \"insurance\",\n",
        "        \"real estate\", \"law firm\", \"courier\", \"cleaning service\"\n",
        "    ],\n",
        "    \"servicios_animales\": [\n",
        "        \"veterinary\", \"pet care\", \"animal hospital\", \"dog park\", \"pet boarding\",\n",
        "        \"dog trainer\", \"animal shelter\"\n",
        "    ],\n",
        "    \"hoteles_y_hospedaje\": [\n",
        "        \"hotel\", \"motel\", \"hostel\", \"bed & breakfast\", \"lodge\", \"resort\",\n",
        "        \"vacation rental\"\n",
        "    ],\n",
        "    \"otros\": []\n",
        "}\n",
        "\n",
        "# Realisamos  un mapeo inverso para búsquedas rápidas\n",
        "inverse_mapping = {}\n",
        "for general_category, keywords in category_mapping.items():\n",
        "    for keyword in keywords:\n",
        "        inverse_mapping[keyword.lower()] = general_category\n",
        "\n",
        "# Función para asignar categorías generales\n",
        "def assign_general_category(category_str):\n",
        "    try:\n",
        "        category_list = ast.literal_eval(category_str)\n",
        "        if isinstance(category_list, list):\n",
        "            for specific_category in category_list:\n",
        "                specific_category_lower = specific_category.lower()\n",
        "                for keyword, general_category in inverse_mapping.items():\n",
        "                    if keyword in specific_category_lower:\n",
        "                        return general_category\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"otros\"\n",
        "\n",
        "# Verificamos las columnas del archivo\n",
        "print(\"Verificando las columnas del archivo...\")\n",
        "sample_data = pd.read_csv(file_path, nrows=5)\n",
        "print(\"Columnas del archivo CSV:\", sample_data.columns.tolist())\n",
        "\n",
        "# Confirmamos que la columna 'category' existe\n",
        "column_name = 'category'  # Nombre correcto de la columna\n",
        "if column_name not in sample_data.columns:\n",
        "    raise KeyError(f\"La columna '{column_name}' no se encuentra en el archivo CSV.\")\n",
        "\n",
        "# Creamos el directorio de salida\n",
        "output_dir = '/content/sample_data/categorias1.1'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Procesamos por lotes para evitar problemas de memoria usando chunk para evitar la sobrecarga de memoria y esta se reinicie\n",
        "chunk_size = 10000  # Tamaño del lote\n",
        "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
        "    chunk['categoria_general'] = chunk[column_name].apply(assign_general_category)\n",
        "\n",
        "    # Guardamos cada categoría en su archivo correspondiente\n",
        "    for general_category, subset in chunk.groupby('categoria_general'):\n",
        "        output_file = f\"{output_dir}/{general_category}.csv\"\n",
        "        if not os.path.exists(output_file):\n",
        "            subset.to_csv(output_file, index=False, mode='w', header=True)\n",
        "        else:\n",
        "            subset.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "print(f\"Archivos separados guardados en: {output_dir}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVFyrdDIrD9d",
        "outputId": "2992d353-7238-4fa8-a29b-95ffc2c59bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verificando las columnas del archivo...\n",
            "Columnas del archivo CSV: ['user_id', 'name_x', 'time', 'rating', 'text', 'pics', 'resp', 'gmap_id', 'name_y', 'address', 'description', 'latitude', 'longitude', 'category', 'avg_rating', 'num_of_reviews', 'price', 'hours', 'MISC', 'state', 'relative_results', 'url']\n",
            "Archivos separados guardados en: /content/sample_data/categorias1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos una limpieza del archivo anterior"
      ],
      "metadata": {
        "id": "AGsitzrfPA80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los directorios de entrada y salida\n",
        "input_dir = '/content/sample_data/categorias1.1'\n",
        "output_dir = '/content/sample_data/categorias1.2'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Procesamos cada archivo en el directorio\n",
        "for filename in os.listdir(input_dir):\n",
        "    if filename.endswith('.csv'):\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        df = pd.read_csv(input_path)\n",
        "\n",
        "        # Eliminamos filas duplicadas\n",
        "        df = df.drop_duplicates()\n",
        "\n",
        "        # Rellenamos datos vacíos con 'Null'\n",
        "        df = df.fillna('Null')\n",
        "\n",
        "        # Guardamos los archivos limpios\n",
        "        df.to_csv(output_path, index=False)\n",
        "\n",
        "        print(f\"Archivo procesado y guardado: {output_path}\")\n",
        "\n",
        "print(\"Todos los archivos han sido procesados y guardados en 'categorias2.5'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhoT2Uhyza5A",
        "outputId": "4b646edc-10da-415d-c956-12f01585a931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/salud.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/otros.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/hoteles_y_hospedaje.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/restaurantes.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/entretenimiento.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/supermercados_tiendas_negocios.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/servicios_animales.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/educacion.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/transporte.csv\n",
            "Archivo procesado y guardado: /content/sample_data/categorias1.2/servicios.csv\n",
            "Todos los archivos han sido procesados y guardados en 'categorias2.5'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos un análisis de sentimientos ala columna text presente en los archivos para saber la satisfacción del cliente en base a sus comentarios estos pueden ser positivo ,negativo o neutral ,también seleccionamos las colunas a usar"
      ],
      "metadata": {
        "id": "fLioaVSwQczP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los directorios de entrada y salida\n",
        "input_dir = '/content/sample_data/categorias1.2'\n",
        "output_dir = '/content/sample_data/categorias3.4'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Función para procesar cada archivo\n",
        "def procesar_archivo(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Eliminamos columnas innecesarias\n",
        "    columnas_a_eliminar = [\n",
        "        'user_id', 'name_x', 'time', 'rating', 'pics', 'resp', 'gmap_id',\n",
        "        'num_of_reviews', 'price', 'hours', 'state', 'relative_results'\n",
        "    ]\n",
        "    df.drop(columns=[col for col in columnas_a_eliminar if col in df.columns], inplace=True)\n",
        "\n",
        "    # Renombramos las columnas a español para una mejor comprención.\n",
        "    df.rename(columns={\n",
        "        'name_y': 'nombre',\n",
        "        'address': 'direccion',\n",
        "        'description': 'descripcion',\n",
        "        'latitude': 'latitud_ciudad',\n",
        "        'longitude': 'longitud_ciudad',\n",
        "        'category': 'categoria',\n",
        "        'avg_rating': 'ranquing_por_usuario'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Procesamos columna 'direccion' para extraer ciudad y código postal basandonos en la separacion por comas\n",
        "    df['ciudad'] = df['direccion'].apply(lambda x: x.split(',')[2].strip() if len(x.split(',')) > 2 else '')\n",
        "    df['codigo_postal_ciudad'] = df['direccion'].apply(lambda x: x.split(',')[3].replace('Fl', '').strip() if len(x.split(',')) > 3 else '')\n",
        "\n",
        "    # Eliminamos 'Fl' de la columna 'codigo_postal_ciudad'\n",
        "    df['codigo_postal_ciudad'] = df['codigo_postal_ciudad'].str.replace('Fl', '').str.strip()\n",
        "\n",
        "    # Ajustamos la  columna 'direccion' eliminando el primer y tercer espacio\n",
        "    df['direccion'] = df['direccion'].apply(lambda x: f\"{x.split(',')[1].strip()}\" if len(x.split(',')) > 1 else '')\n",
        "\n",
        "    # Realizamos el analisis desentimientos a la columna 'text'\n",
        "    def analizar_sentimiento(texto):\n",
        "        try:\n",
        "            sentimiento = TextBlob(str(texto)).sentiment.polarity\n",
        "            if sentimiento > 0:\n",
        "                return 'positivo'\n",
        "            elif sentimiento < 0:\n",
        "                return 'negativo'\n",
        "            else:\n",
        "                return 'neutral'\n",
        "        except:\n",
        "            return 'neutral'\n",
        "\n",
        "    df['analisis_sentimientos'] = df['text'].apply(analizar_sentimiento)\n",
        "\n",
        "    # Eliminamos la columna 'text'\n",
        "    df.drop(columns=['text'], inplace=True)\n",
        "\n",
        "    # Guardamos el archivo procesado\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "# Procesamos todos los archivos en el directorio de entrada\n",
        "for file_name in os.listdir(input_dir):\n",
        "    if file_name.endswith('.csv'):\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "        output_path = os.path.join(output_dir, file_name)\n",
        "        procesar_archivo(input_path, output_path)\n",
        "\n",
        "print(f\"Archivos procesados y guardados en: {output_dir}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EcWoc5SlzjpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc376134-7967-4d58-dda2-004fed876916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos procesados y guardados en: /content/sample_data/categorias3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante el archivo de dimensiones ciudades_dim rellenamos la información correspondiente a cada dato para ciudades y condados"
      ],
      "metadata": {
        "id": "FZj1E1EqSGsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los directorios y archivos\n",
        "input_dir = '/content/sample_data/categorias3.4'\n",
        "output_dir = '/content/sample_data/categorias3.5_procesado'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Cargamos el archivo de referencia para llenar las nuevas columnas\n",
        "ciudades_dim_path = '/content/sample_data/ciudades_dim.csv'\n",
        "ciudades_dim = pd.read_csv(ciudades_dim_path)\n",
        "\n",
        "# Procesamos cada archivo en el directorio\n",
        "def procesar_archivo(file_path, output_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Mostramos las columnas disponibles para depuración\n",
        "    print(f\"Procesando archivo: {file_path}\")\n",
        "    print(\"Columnas disponibles:\", df.columns.tolist())\n",
        "    if 'ciudad' not in df.columns:\n",
        "        raise ValueError(f\"El archivo {file_path} no contiene la columna 'ciudad'.\")\n",
        "\n",
        "    # realisamos el merge con el archivo ciudades_dim para obtener id_ciudad y datos relacionados\n",
        "    df = df.merge(\n",
        "        ciudades_dim[['ciudad', 'id_ciudad', 'id_condado', 'condado', 'codigo_postal_condado', 'latitud_condado', 'longitud_condado']],\n",
        "        on='ciudad',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Renombramos la columna MISC a atributo\n",
        "    if 'MISC' in df.columns:\n",
        "        df.rename(columns={'MISC': 'atributo'}, inplace=True)\n",
        "\n",
        "    # Eliminamos \"FL\" de la columna codigo_postal_ciudad\n",
        "    if 'codigo_postal_ciudad' in df.columns:\n",
        "        df['codigo_postal_ciudad'] = df['codigo_postal_ciudad'].astype(str).str.replace(r'\\bFL\\b', '', regex=True).str.strip()\n",
        "\n",
        "    # Agregamos columnas de anio y estado\n",
        "    df['estado'] = 'Florida'\n",
        "    df['anio'] = 2024\n",
        "\n",
        "    # Eliminamos filas duplicadas\n",
        "    df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Ordenamos las  columnas en el orden especificado\n",
        "    columnas_orden = [\n",
        "        'id_nombre', 'nombre', 'direccion', 'id_condado', 'condado', 'codigo_postal_condado',\n",
        "        'latitud_condado', 'longitud_condado', 'id_ciudad', 'ciudad', 'codigo_postal_ciudad',\n",
        "        'latitud_ciudad', 'longitud_ciudad', 'estado', 'atributo',\n",
        "        'categoria', 'ranquing_por_usuario', 'numero_de_reviews', 'analisis_sentimientos',\n",
        "        'url', 'enlaces_google_maps', 'anio'\n",
        "    ]\n",
        "\n",
        "    # Rellenamos los datos faltantes con NaN\n",
        "    for col in columnas_orden:\n",
        "        if col not in df.columns:\n",
        "            df[col] = None\n",
        "\n",
        "    # Reordenamos  las columnas\n",
        "    df = df[columnas_orden]\n",
        "\n",
        "    # Guardamos el archivo procesado\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "# Procesamos todos los archivos en el directorio de entrada\n",
        "for file_name in os.listdir(input_dir):\n",
        "    if file_name.endswith('.csv'):\n",
        "        input_path = os.path.join(input_dir, file_name)\n",
        "        output_path = os.path.join(output_dir, file_name)\n",
        "        procesar_archivo(input_path, output_path)\n",
        "\n",
        "print(f\"Archivos procesados y guardados en: {output_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEv0La5qM0FJ",
        "outputId": "1211eee4-a28e-40a8-aee2-789d67b78e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando archivo: /content/sample_data/categorias3.4/salud.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/otros.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/hoteles_y_hospedaje.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/restaurantes.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/entretenimiento.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/supermercados_tiendas_negocios.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/servicios_animales.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/educacion.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/transporte.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Procesando archivo: /content/sample_data/categorias3.4/servicios.csv\n",
            "Columnas disponibles: ['nombre', 'direccion', 'descripcion', 'latitud_ciudad', 'longitud_ciudad', 'categoria', 'ranquing_por_usuario', 'MISC', 'url', 'categoria_general', 'ciudad', 'codigo_postal_ciudad', 'analisis_sentimientos']\n",
            "Archivos procesados y guardados en: /content/sample_data/categorias3.5_procesado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada local establecimiento del sevicio de educación"
      ],
      "metadata": {
        "id": "WoHwuaZ5U_I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los archivos de entrada y salida\n",
        "educacion_path = '/content/sample_data/categorias3.5_procesado/educacion.csv'\n",
        "unidades_educativas_path = '/content/drive/MyDrive/categorias3.3/centros_educacion_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "educacion_df = pd.read_csv(educacion_path)\n",
        "unidades_educativas_df = pd.read_csv(unidades_educativas_path)\n",
        "\n",
        "# Cambinamos los nombres de las columnas\n",
        "educacion_df.rename(columns={'nombre': 'centros_educativos', 'id_nombre': 'id_centros_educativos'}, inplace=True)\n",
        "unidades_educativas_df.rename(columns={'nombre': 'centros_educativos', 'id_nombre': 'id_centros_educativos'}, inplace=True)\n",
        "educacion_df['id_centros_educativos'] = educacion_df['id_centros_educativos'].astype(str)\n",
        "\n",
        "# Función para normalizar nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "\n",
        "# Normalizamos los nombres en ambas tablas para evitar problemas con asentos y mayusculas\n",
        "educacion_df['centros_educativos_normalizado'] = educacion_df['centros_educativos'].apply(normalizar_nombre)\n",
        "unidades_educativas_df['centros_educativos_normalizado'] = unidades_educativas_df['centros_educativos'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar un ids alfanumérico único de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "\n",
        "# Creamos un conjunto de ids existentes para asegurar unicidad\n",
        "ids_existentes = set(unidades_educativas_df['id_centros_educativos'])\n",
        "\n",
        "# Procesamos los centros educativos y completamos id_centros_educativos com los nombres nuevos\n",
        "nuevas_unidades = []\n",
        "for index, row in educacion_df.iterrows():\n",
        "    nombre_norm = row['centros_educativos_normalizado']\n",
        "    # Buscamos en unidades_educativas_dim\n",
        "    matching_row = unidades_educativas_df[unidades_educativas_df['centros_educativos_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        educacion_df.at[index, 'id_centros_educativos'] = matching_row['id_centros_educativos'].values[0]\n",
        "    else:\n",
        "        # Generamos un nuevo id para los datos nuevos\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        educacion_df.at[index, 'id_centros_educativos'] = nuevo_id\n",
        "        nuevas_unidades.append({'id_centros_educativos': nuevo_id, 'centros_educativos': row['centros_educativos'], 'centros_educativos_normalizado': nombre_norm})\n",
        "\n",
        "# Añadimos nuevas unidades al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "  #Nos aceguramos que no haya duplicados antes de concatenar\n",
        "    unidades_educativas_df = pd.concat([unidades_educativas_df, nuevas_unidades_df]).drop_duplicates(subset='centros_educativos_normalizado', keep='first')\n",
        "    unidades_educativas_df.drop(columns=['centros_educativos_normalizado'], inplace=True)\n",
        "    unidades_educativas_df.to_csv(unidades_educativas_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevas unidades educativas al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de educación en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'servicios_educacion.csv')\n",
        "educacion_df.drop(columns=['centros_educativos_normalizado'], inplace=True)\n",
        "educacion_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEd3DQlKP6eK",
        "outputId": "4ec13da3-8cd3-4a81-ec22-08f2fdf6dad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han añadido 2140 nuevas unidades educativas al archivo de referencia.\n",
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/servicios_educacion.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada local establecimiento de servicio de entretenimiento"
      ],
      "metadata": {
        "id": "9k7qeIhEVA5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos lo archivos de entrada y salida\n",
        "entretenimiento_path = '/content/sample_data/categorias3.5_procesado/entretenimiento.csv'\n",
        "centros_entretenimiento_path = '/content/drive/MyDrive/categorias3.3/centros_entretenimiento_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "entretenimiento_df = pd.read_csv(entretenimiento_path)\n",
        "centros_entretenimiento_df = pd.read_csv(centros_entretenimiento_path)\n",
        "\n",
        "# Cambiamos los nombres de las columnas\n",
        "entretenimiento_df.rename(columns={'nombre': 'centros_entretenimiento', 'id_nombre': 'id_centros_entretenimiento'}, inplace=True)\n",
        "centros_entretenimiento_df.rename(columns={'nombre': 'centros_entretenimiento', 'id_nombre': 'id_centros_entretenimiento'}, inplace=True)\n",
        "entretenimiento_df['id_centros_entretenimiento'] = entretenimiento_df['id_centros_entretenimiento'].astype(str)\n",
        "\n",
        "# Función para normalizar nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "\n",
        "# Normalizamos los nombres en ambas tablas\n",
        "entretenimiento_df['centros_entretenimiento_normalizado'] = entretenimiento_df['centros_entretenimiento'].apply(normalizar_nombre)\n",
        "centros_entretenimiento_df['centros_entretenimiento_normalizado'] = centros_entretenimiento_df['centros_entretenimiento'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar un ids alfanumérico único de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "ids_existentes = set(centros_entretenimiento_df['id_centros_entretenimiento'])\n",
        "\n",
        "# Procesamos los centros de entretenimiento y completamos id_centros_entretenimiento\n",
        "nuevas_unidades = []\n",
        "for index, row in entretenimiento_df.iterrows():\n",
        "    nombre_norm = row['centros_entretenimiento_normalizado']\n",
        "    matching_row = centros_entretenimiento_df[centros_entretenimiento_df['centros_entretenimiento_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        entretenimiento_df.at[index, 'id_centros_entretenimiento'] = matching_row['id_centros_entretenimiento'].values[0]\n",
        "    else:\n",
        "        # Generamos nuevas ids para los nuevos nombres\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        entretenimiento_df.at[index, 'id_centros_entretenimiento'] = nuevo_id\n",
        "        nuevas_unidades.append({'id_centros_entretenimiento': nuevo_id, 'centros_entretenimiento': row['centros_entretenimiento'], 'centros_entretenimiento_normalizado': nombre_norm})\n",
        "\n",
        "# Eliminamos filas duplicadas en entretenimiento_df basados en todas las columnas\n",
        "entretenimiento_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Añadimos nuevos establecimientos al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "    # Nos aseguramos que no haya duplicados antes de concatenar\n",
        "    centros_entretenimiento_df = pd.concat([centros_entretenimiento_df, nuevas_unidades_df]).drop_duplicates(subset='centros_entretenimiento_normalizado', keep='first')\n",
        "    centros_entretenimiento_df.drop(columns=['centros_entretenimiento_normalizado'], inplace=True)\n",
        "    centros_entretenimiento_df.to_csv(centros_entretenimiento_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevos centros de entretenimiento al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de entretenimiento en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'servicios_entretenimiento.csv')\n",
        "entretenimiento_df.drop(columns=['centros_entretenimiento_normalizado'], inplace=True)\n",
        "entretenimiento_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Lh-5151W8U",
        "outputId": "901e3c4a-b29b-478a-e298-0c52f3990415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han añadido 786 nuevos centros de entretenimiento al archivo de referencia.\n",
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/servicios_entretenimiento.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada local establecimiento de servicios de hoteles_y_hospedaje"
      ],
      "metadata": {
        "id": "hD1UggbHVCUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Archivos de entrada y salida\n",
        "hosteleria_path = '/content/sample_data/categorias3.5_procesado/hoteles_y_hospedaje.csv'\n",
        "centros_hosteleria_path = '/content/drive/MyDrive/categorias3.3/centros_hosteleria_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "hosteleria_df = pd.read_csv(hosteleria_path)\n",
        "centros_hosteleria_df = pd.read_csv(centros_hosteleria_path)\n",
        "\n",
        "# Cambiamos los nombres de las columnas\n",
        "hosteleria_df.rename(columns={'nombre': 'centros_hosteleria', 'id_nombre': 'id_centros_hosteleria'}, inplace=True)\n",
        "centros_hosteleria_df.rename(columns={'nombre': 'centros_hosteleria', 'id_nombre': 'id_centros_hosteleria'}, inplace=True)\n",
        "hosteleria_df['id_centros_hosteleria'] = hosteleria_df['id_centros_hosteleria'].astype(str)\n",
        "\n",
        "# Función para normalizar los nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "\n",
        "# Normalizamos los nombres en ambas tablas\n",
        "hosteleria_df['centros_hosteleria_normalizado'] = hosteleria_df['centros_hosteleria'].apply(normalizar_nombre)\n",
        "centros_hosteleria_df['centros_hosteleria_normalizado'] = centros_hosteleria_df['centros_hosteleria'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar ids alfanumérico únicos de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "ids_existentes = set(centros_hosteleria_df['id_centros_hosteleria'])\n",
        "\n",
        "# Procesamos los centros de hostelería y completamos las id_centros_hosteleria\n",
        "nuevas_unidades = []\n",
        "for index, row in hosteleria_df.iterrows():\n",
        "    nombre_norm = row['centros_hosteleria_normalizado']\n",
        "    matching_row = centros_hosteleria_df[centros_hosteleria_df['centros_hosteleria_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        hosteleria_df.at[index, 'id_centros_hosteleria'] = matching_row['id_centros_hosteleria'].values[0]\n",
        "    else:\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        hosteleria_df.at[index, 'id_centros_hosteleria'] = nuevo_id\n",
        "        nuevas_unidades.append({'id_centros_hosteleria': nuevo_id, 'centros_hosteleria': row['centros_hosteleria'], 'centros_hosteleria_normalizado': nombre_norm})\n",
        "\n",
        "# Eliminamos filas duplicadas en hosteleria_df basados en todas las columnas\n",
        "hosteleria_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Añadimos nuevos nombres al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "    centros_hosteleria_df = pd.concat([centros_hosteleria_df, nuevas_unidades_df]).drop_duplicates(subset='centros_hosteleria_normalizado', keep='first')\n",
        "    centros_hosteleria_df.drop(columns=['centros_hosteleria_normalizado'], inplace=True)\n",
        "    centros_hosteleria_df.to_csv(centros_hosteleria_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevos centros de hostelería al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de hostelería en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'servicios_hosteleria.csv')\n",
        "hosteleria_df.drop(columns=['centros_hosteleria_normalizado'], inplace=True)\n",
        "hosteleria_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9YlICfR1W5J",
        "outputId": "813c334e-3a39-4b5a-b86a-481e0e887c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/servicios_hosteleria.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada local establecimiento"
      ],
      "metadata": {
        "id": "DDu5PhE8VEIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los archivos de entrada y salida\n",
        "restaurantes_path = '/content/sample_data/categorias3.5_procesado/restaurantes.csv'\n",
        "centros_restaurantes_path = '/content/drive/MyDrive/categorias3.3/centros_restaurantes_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "restaurantes_df = pd.read_csv(restaurantes_path)\n",
        "centros_restaurantes_df = pd.read_csv(centros_restaurantes_path)\n",
        "\n",
        "# Cambiamos los nombres de las columnas\n",
        "restaurantes_df.rename(columns={'nombre': 'centros_restaurantes', 'id_nombre': 'id_centros_restaurantes'}, inplace=True)\n",
        "centros_restaurantes_df.rename(columns={'nombre': 'centros_restaurantes', 'id_nombre': 'id_centros_restaurantes'}, inplace=True)\n",
        "restaurantes_df['id_centros_restaurantes'] = restaurantes_df['id_centros_restaurantes'].astype(str)\n",
        "\n",
        "# Función para normalizar nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "\n",
        "# Normalizamos los nombres en ambas tablas\n",
        "restaurantes_df['centros_restaurantes_normalizado'] = restaurantes_df['centros_restaurantes'].apply(normalizar_nombre)\n",
        "centros_restaurantes_df['centros_restaurantes_normalizado'] = centros_restaurantes_df['centros_restaurantes'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar ids alfanuméricos únicos de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "ids_existentes = set(centros_restaurantes_df['id_centros_restaurantes'])\n",
        "\n",
        "# Procesamos los centros de restaurantes y completar id_centros_restaurantes\n",
        "nuevas_unidades = []\n",
        "for index, row in restaurantes_df.iterrows():\n",
        "    nombre_norm = row['centros_restaurantes_normalizado']\n",
        "    matching_row = centros_restaurantes_df[centros_restaurantes_df['centros_restaurantes_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        restaurantes_df.at[index, 'id_centros_restaurantes'] = matching_row['id_centros_restaurantes'].values[0]\n",
        "    else:\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        restaurantes_df.at[index, 'id_centros_restaurantes'] = nuevo_id\n",
        "        nuevas_unidades.append({\n",
        "            'id_centros_restaurantes': nuevo_id,\n",
        "            'centros_restaurantes': row['centros_restaurantes'],\n",
        "            'centros_restaurantes_normalizado': nombre_norm\n",
        "        })\n",
        "\n",
        "# Eliminamos filas duplicadas en restaurantes_df basadas en todas las columnas\n",
        "restaurantes_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Añadimos nuevos nombres al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "    centros_restaurantes_df = pd.concat([centros_restaurantes_df, nuevas_unidades_df]).drop_duplicates(subset='centros_restaurantes_normalizado', keep='first')\n",
        "    centros_restaurantes_df.drop(columns=['centros_restaurantes_normalizado'], inplace=True)\n",
        "    centros_restaurantes_df.to_csv(centros_restaurantes_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevos centros de restaurantes al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de restaurantes en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'restaurantes_actualizado.csv')\n",
        "restaurantes_df.drop(columns=['centros_restaurantes_normalizado'], inplace=True)\n",
        "restaurantes_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaGCUJk51W2u",
        "outputId": "5b25d277-6d72-414e-ef5c-fa88dc464bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han añadido 15663 nuevos centros de restaurantes al archivo de referencia.\n",
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/restaurantes_actualizado.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada establesimiendo de supermercados_tiendas_negocios"
      ],
      "metadata": {
        "id": "VgVLJuxxVHCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Archivos de entrada y salida\n",
        "tiendas_negocio_path = '/content/sample_data/categorias3.5_procesado/supermercados_tiendas_negocios.csv'\n",
        "centros_tiendas_negocio_path = '/content/drive/MyDrive/categorias3.3/servicio_tiendas_negocio_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "tiendas_negocio_df = pd.read_csv(tiendas_negocio_path)\n",
        "centros_tiendas_negocio_df = pd.read_csv(centros_tiendas_negocio_path)\n",
        "\n",
        "# Cambiamos los nombres de las columnas\n",
        "tiendas_negocio_df.rename(columns={'nombre': 'centros_tiendas_negocio', 'id_nombre': 'id_centros_tiendas_negocio'}, inplace=True)\n",
        "centros_tiendas_negocio_df.rename(columns={'nombre': 'centros_tiendas_negocio', 'id_nombre': 'id_centros_tiendas_negocio'}, inplace=True)\n",
        "tiendas_negocio_df['id_centros_tiendas_negocio'] = tiendas_negocio_df['id_centros_tiendas_negocio'].astype(str)\n",
        "\n",
        "# Función para normalizar los nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "\n",
        "# Normalizamos los nombres en ambas tablas\n",
        "tiendas_negocio_df['centros_tiendas_negocio_normalizado'] = tiendas_negocio_df['centros_tiendas_negocio'].apply(normalizar_nombre)\n",
        "centros_tiendas_negocio_df['centros_tiendas_negocio_normalizado'] = centros_tiendas_negocio_df['centros_tiendas_negocio'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar ids alfanuméricos únicos de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "\n",
        "# Creamos un conjunto de ids existentes para asegurar unicidad\n",
        "ids_existentes = set(centros_tiendas_negocio_df['id_centros_tiendas_negocio'])\n",
        "\n",
        "# Procesamos los centros de tiendas y negocios y completamos id_centros_tiendas_negocio\n",
        "nuevas_unidades = []\n",
        "for index, row in tiendas_negocio_df.iterrows():\n",
        "    nombre_norm = row['centros_tiendas_negocio_normalizado']\n",
        "    matching_row = centros_tiendas_negocio_df[centros_tiendas_negocio_df['centros_tiendas_negocio_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        tiendas_negocio_df.at[index, 'id_centros_tiendas_negocio'] = matching_row['id_centros_tiendas_negocio'].values[0]\n",
        "    else:\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        tiendas_negocio_df.at[index, 'id_centros_tiendas_negocio'] = nuevo_id\n",
        "        nuevas_unidades.append({\n",
        "            'id_centros_tiendas_negocio': nuevo_id,\n",
        "            'centros_tiendas_negocio': row['centros_tiendas_negocio'],\n",
        "            'centros_tiendas_negocio_normalizado': nombre_norm\n",
        "        })\n",
        "\n",
        "# Eliminamos filas duplicadas en tiendas_negocio_df basadas en todas las columnas\n",
        "tiendas_negocio_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Añadimos nuevas unidades al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "    centros_tiendas_negocio_df = pd.concat([centros_tiendas_negocio_df, nuevas_unidades_df]).drop_duplicates(subset='centros_tiendas_negocio_normalizado', keep='first')\n",
        "    centros_tiendas_negocio_df.drop(columns=['centros_tiendas_negocio_normalizado'], inplace=True)\n",
        "    centros_tiendas_negocio_df.to_csv(centros_tiendas_negocio_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevos centros de tiendas y negocios al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de tiendas y negocios en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'tiendas_negocio_actualizado.csv')\n",
        "tiendas_negocio_df.drop(columns=['centros_tiendas_negocio_normalizado'], inplace=True)\n",
        "tiendas_negocio_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGGZo4zY1WxC",
        "outputId": "2f6c5ae6-4674-4f50-f194-979faa93e0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han añadido 7193 nuevos centros de tiendas y negocios al archivo de referencia.\n",
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/tiendas_negocio_actualizado.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mediante la tabla de dimensiones correspondiente otorgamos la id correspondiente a cada establecimiento de trasporte"
      ],
      "metadata": {
        "id": "K2lR2K-zVISD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los archivos de entrada y salida\n",
        "transporte_data_path = '/content/sample_data/categorias3.5_procesado/transporte.csv'\n",
        "centros_transporte_dim_path = '/content/drive/MyDrive/categorias3.3/servicio_transporte_dim.csv'\n",
        "output_dir = '/content/sample_data/categorias3.6'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "transporte_df = pd.read_csv(transporte_data_path)\n",
        "centros_transporte_df = pd.read_csv(centros_transporte_dim_path)\n",
        "\n",
        "# Cambiamos los nombres de las columnas\n",
        "transporte_df.rename(columns={'nombre': 'transporte', 'id_nombre': 'id_transporte'}, inplace=True)\n",
        "centros_transporte_df.rename(columns={'nombre': 'transporte', 'id_nombre': 'id_transporte'}, inplace=True)\n",
        "transporte_df['id_transporte'] = transporte_df['id_transporte'].astype(str)\n",
        "\n",
        "# Función para normalizar los nombres\n",
        "def normalizar_nombre(nombre):\n",
        "    if pd.isna(nombre):\n",
        "        return nombre\n",
        "    nombre = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", str(nombre))\n",
        "    return nombre.strip().lower()\n",
        "# Normalizamos los nombres en ambas tablas\n",
        "transporte_df['transporte_normalizado'] = transporte_df['transporte'].apply(normalizar_nombre)\n",
        "centros_transporte_df['transporte_normalizado'] = centros_transporte_df['transporte'].apply(normalizar_nombre)\n",
        "\n",
        "# Función para generar un ids alfanuméricos únicos de 5 caracteres\n",
        "def generar_id_unico(existentes):\n",
        "    while True:\n",
        "        nuevo_id = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))\n",
        "        if nuevo_id not in existentes:\n",
        "            return nuevo_id\n",
        "ids_existentes = set(centros_transporte_df['id_transporte'])\n",
        "\n",
        "# Procesar los centros de transporte y completar id_transporte\n",
        "nuevas_unidades = []\n",
        "for index, row in transporte_df.iterrows():\n",
        "    nombre_norm = row['transporte_normalizado']\n",
        "    matching_row = centros_transporte_df[centros_transporte_df['transporte_normalizado'] == nombre_norm]\n",
        "    if not matching_row.empty:\n",
        "        transporte_df.at[index, 'id_transporte'] = matching_row['id_transporte'].values[0]\n",
        "    else:\n",
        "        nuevo_id = generar_id_unico(ids_existentes)\n",
        "        ids_existentes.add(nuevo_id)\n",
        "        transporte_df.at[index, 'id_transporte'] = nuevo_id\n",
        "        nuevas_unidades.append({\n",
        "            'id_transporte': nuevo_id,\n",
        "            'transporte': row['transporte'],\n",
        "            'transporte_normalizado': nombre_norm\n",
        "        })\n",
        "\n",
        "# Eliminamos las filas duplicadas en transporte_df basados en todas las columnas\n",
        "transporte_df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Añadimos nuevos nombres al archivo de referencia\n",
        "if nuevas_unidades:\n",
        "    nuevas_unidades_df = pd.DataFrame(nuevas_unidades)\n",
        "    centros_transporte_df = pd.concat([centros_transporte_df, nuevas_unidades_df]).drop_duplicates(subset='transporte_normalizado', keep='first')\n",
        "    centros_transporte_df.drop(columns=['transporte_normalizado'], inplace=True)\n",
        "    centros_transporte_df.to_csv(centros_transporte_dim_path, index=False)\n",
        "    print(f\"Se han añadido {len(nuevas_unidades)} nuevos centros de transporte al archivo de referencia.\")\n",
        "\n",
        "# Guardamos el archivo actualizado de transporte en el directorio de salida\n",
        "output_path = os.path.join(output_dir, 'transporte_actualizado.csv')\n",
        "transporte_df.drop(columns=['transporte_normalizado'], inplace=True)\n",
        "transporte_df.to_csv(output_path, index=False)\n",
        "print(f\"Archivo actualizado guardado en: {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEBGQ7ZM1Ws5",
        "outputId": "fa821394-67dc-4146-d26e-200e66bfe654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se han añadido 1120 nuevos centros de transporte al archivo de referencia.\n",
            "Archivo actualizado guardado en: /content/sample_data/categorias3.6/transporte_actualizado.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6PY4NnJHfK1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Ruta de origen y destino\n",
        "origen = \"/content/sample_data/categorias3.6\"\n",
        "destino = \"/content/drive/MyDrive/categorias10\"\n",
        "\n",
        "# Verificar si el destino existe y eliminarlo si es necesario\n",
        "if os.path.exists(destino):\n",
        "    shutil.rmtree(destino)  # Eliminar el directorio existente\n",
        "\n",
        "# Copiar el directorio\n",
        "shutil.copytree(origen, destino)\n",
        "print(f\"Directorio copiado de {origen} a {destino}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRMsQTxPtaoL",
        "outputId": "2587fadf-d641-4fe8-9044-43ec66c492c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directorio copiado de /content/sample_data/categorias3.6 a /content/drive/MyDrive/categorias10\n"
          ]
        }
      ]
    }
  ]
}