{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "import pandas as pd\n",
    "import numpy as ny\n",
    "import googlemaps\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderInsufficientPrivileges\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (362, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           169\n",
      "addr:street    261\n",
      "addr:city      268\n",
      "addr:state     283\n",
      "amenity        362\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  362.000000  362.000000\n",
      "mean       NaN   27.694528  -81.558128\n",
      "std        NaN    1.443474    1.304508\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.271788  -82.135901\n",
      "50%        NaN   27.936811  -81.558859\n",
      "75%        NaN   28.552715  -80.339784\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           188\n",
      "addr:street     88\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           362\n",
      "@lon           362\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2014.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos las direcciones en base ala latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3                      None   \n",
      "4                  CocoWalk   \n",
      "\n",
      "                                         addr:street   addr:city addr:state  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...  Titusville        NaN   \n",
      "1  Landmark Shopping Center, North 20th Street, H...        None        NaN   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...        None        NaN   \n",
      "3      Manatee County, Florida, 34222, United States        None        NaN   \n",
      "4  CocoWalk, 3015, Grand Avenue, Dinner Key, Miam...       Miami        NaN   \n",
      "\n",
      "   amenity       @lat       @lon  \n",
      "0      NaN  28.575472 -80.802640  \n",
      "1      NaN  28.068730 -82.436298  \n",
      "2      NaN  28.068711 -82.437068  \n",
      "3      NaN  27.534335 -82.506753  \n",
      "4      NaN  25.728644 -80.242038  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2014.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicamos la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardamos los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2014.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos colunas innecesarias y cambiamos los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                  direccion  \\\n",
      "0           Titusville Mall                  Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center     North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza  East Fletcher Avenue, Hillsborough County   \n",
      "3            Manatee County                             Florida, 34222   \n",
      "4                  CocoWalk                         3015, Grand Avenue   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3       Manatee County  Florida  United States         34222 -82.506753   \n",
      "4    Miami-Dade County  Florida  United States         33133 -80.242038   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.534335  \n",
      "4  25.728644  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2014.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2014.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra Couty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            direccion        ciudad\n",
      "0           Narvaez Drive, Titusville       Brevard\n",
      "1     North 20th Street, Hillsborough  Hillsborough\n",
      "2  East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3                      Florida, 34222       Manatee\n",
      "4                  3015, Grand Avenue    Miami-Dade\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2014.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las ids para nombre y ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2014.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2014_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el area de ML se combierte el archivo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2014_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2014_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2014_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (362, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           169\n",
      "addr:street    261\n",
      "addr:city      268\n",
      "addr:state     283\n",
      "amenity        362\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  362.000000  362.000000\n",
      "mean       NaN   27.694528  -81.558128\n",
      "std        NaN    1.443474    1.304508\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.271788  -82.135901\n",
      "50%        NaN   27.936811  -81.558859\n",
      "75%        NaN   28.552715  -80.339784\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           188\n",
      "addr:street     88\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           362\n",
      "@lon           362\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2015.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos las direcciones en base ala latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3                      None   \n",
      "4                  CocoWalk   \n",
      "\n",
      "                                         addr:street   addr:city addr:state  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...  Titusville        NaN   \n",
      "1  Landmark Shopping Center, North 20th Street, H...        None        NaN   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...        None        NaN   \n",
      "3      Manatee County, Florida, 34222, United States        None        NaN   \n",
      "4  CocoWalk, 3015, Grand Avenue, Dinner Key, Miam...       Miami        NaN   \n",
      "\n",
      "   amenity       @lat       @lon  \n",
      "0      NaN  28.575472 -80.802640  \n",
      "1      NaN  28.068730 -82.436298  \n",
      "2      NaN  28.068711 -82.437068  \n",
      "3      NaN  27.534335 -82.506753  \n",
      "4      NaN  25.728644 -80.242038  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2015.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicamos la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardamos los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2015.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos colunas innecesarias y cambiamos los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                  direccion  \\\n",
      "0           Titusville Mall                  Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center     North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza  East Fletcher Avenue, Hillsborough County   \n",
      "3            Manatee County                             Florida, 34222   \n",
      "4                  CocoWalk                         3015, Grand Avenue   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3       Manatee County  Florida  United States         34222 -82.506753   \n",
      "4    Miami-Dade County  Florida  United States         33133 -80.242038   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.534335  \n",
      "4  25.728644  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2015.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2015.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            direccion        ciudad\n",
      "0           Narvaez Drive, Titusville       Brevard\n",
      "1     North 20th Street, Hillsborough  Hillsborough\n",
      "2  East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3                      Florida, 34222       Manatee\n",
      "4                  3015, Grand Avenue    Miami-Dade\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2015.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las ids para nombre y ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2015.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2015_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el area de ML se combierte el archivo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2015_final.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2015_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2015_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (348, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           156\n",
      "addr:street    247\n",
      "addr:city      254\n",
      "addr:state     269\n",
      "amenity        348\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  348.000000  348.000000\n",
      "mean       NaN   27.609325  -81.547966\n",
      "std        NaN    1.398079    1.328651\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.254266  -82.176331\n",
      "50%        NaN   27.906197  -81.459963\n",
      "75%        NaN   28.467118  -80.337658\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           187\n",
      "addr:street     88\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           348\n",
      "@lon           348\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos las direcciones en base ala latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        @lat       @lon                                        addr:street  \\\n",
      "0  28.575472 -80.802640  Titusville Mall, Narvaez Drive, Titusville, Br...   \n",
      "1  28.068730 -82.436298  Landmark Shopping Center, North 20th Street, H...   \n",
      "2  28.068711 -82.437068  LandMark Plaza, East Fletcher Avenue, Hillsbor...   \n",
      "3  27.534335 -82.506753      Manatee County, Florida, 34222, United States   \n",
      "4  25.728644 -80.242038  CocoWalk, 3015, Grand Avenue, Dinner Key, Miam...   \n",
      "\n",
      "    addr:city  \n",
      "0  Titusville  \n",
      "1        None  \n",
      "2        None  \n",
      "3        None  \n",
      "4       Miami  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            # Extraemos  dirección completa y ciudad\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            return address, city\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None\n",
    "\n",
    "# Actualizamos las columnas addr:street y addr:city\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address  # Guardar dirección completa\n",
    "        row['addr:city'] = city       # Actualizar ciudad\n",
    "    return row\n",
    "\n",
    "# Aplicamos una pausa de 2 segundos fila por fila con retraso para respetar los límites de solisitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardamos los resultados en un nuevo archivo CSV\n",
    "data.to_csv(\"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv\", index=False)\n",
    "print(data[['@lat', '@lon', 'addr:street', 'addr:city']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos colunas innecesarias y cambiamos los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "latitud          0\n",
      "longitud         0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                 direccion     latitud  \\\n",
      "0           Titusville Mall           Titusville Mall  28.5754724   \n",
      "1  Landmark Shopping Center  Landmark Shopping Center  28.0687299   \n",
      "2            LandMark Plaza            LandMark Plaza   28.068711   \n",
      "3  Ellenton Premium Outlets            Manatee County  27.5343346   \n",
      "4                  CocoWalk                  CocoWalk  25.7286438   \n",
      "\n",
      "      longitud                ciudad   estado           pais codigo_postal  \n",
      "0    -80.80264            Titusville  Florida  United States         32780  \n",
      "1   -82.436298     North 20th Street  Florida  United States         33613  \n",
      "2  -82.4370678  East Fletcher Avenue  Florida  United States         33613  \n",
      "3   -82.506753                   nan  Florida  United States         34222  \n",
      "4  -80.2420384                 Miami  Florida  United States         33133  \n"
     ]
    }
   ],
   "source": [
    "# Cambiamos nombres de columnas para mejor comprencion\n",
    "data.rename(columns={\n",
    "    'name': 'nombre',\n",
    "    'addr:street': 'direccion',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Creamos nuevas columnas para ciudad, estado, país y código postal\n",
    "data['ciudad'] = data['direccion'].str.extract(r', ([^,]+), [^,]+ County')[0]\n",
    "data['estado'] = data['direccion'].str.extract(r', ([^,]+), \\d{5}, United States')[0]\n",
    "data['pais'] = 'United States'  # Dado que todos los datos son de EE.UU.\n",
    "data['codigo_postal'] = data['direccion'].str.extract(r', (\\d{5}), United States')[0]\n",
    "\n",
    "# Limpiamos la columna 'direccion' para que quede solo la dirección\n",
    "data['direccion'] = data['direccion'].str.extract(r'^([^,]+)')[0]\n",
    "\n",
    "# Convertimos todas las columnas a texto para garantizar compatibilidad\n",
    "data = data.astype(str)\n",
    "\n",
    "# Rellenamos valores faltantes con 'None'\n",
    "data.fillna('None', inplace=True)\n",
    "\n",
    "# Eliminamos columnas no deseadas\n",
    "data.drop(columns=['addr:city', 'addr:state', 'amenity'], inplace=True, errors='ignore')\n",
    "\n",
    "# Eliminamos duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el archivo procesado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Realizamos un análisis de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las ids para nombre y ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargar catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardar catálogos\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generar IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargar catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesar archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validar columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Columnas de salida\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # Escribir archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "# Ejecutar procesamiento\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardar catálogos actualizados\n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (337, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           148\n",
      "addr:street    238\n",
      "addr:city      244\n",
      "addr:state     259\n",
      "amenity        337\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  337.000000  337.000000\n",
      "mean       NaN   27.568682  -81.530013\n",
      "std        NaN    1.369453    1.308599\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.248097  -82.186697\n",
      "50%        NaN   27.873625  -81.444381\n",
      "75%        NaN   28.449604  -80.337432\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           184\n",
      "addr:street     86\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           337\n",
      "@lon           337\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2017.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3                      None   \n",
      "4                  CocoWalk   \n",
      "\n",
      "                                         addr:street   addr:city addr:state  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...  Titusville        NaN   \n",
      "1  Landmark Shopping Center, North 20th Street, H...        None        NaN   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...        None        NaN   \n",
      "3      Manatee County, Florida, 34222, United States        None        NaN   \n",
      "4  CocoWalk, 3015, Grand Avenue, Dinner Key, Miam...       Miami        NaN   \n",
      "\n",
      "   amenity       @lat       @lon  \n",
      "0      NaN  28.575472 -80.802640  \n",
      "1      NaN  28.068730 -82.436298  \n",
      "2      NaN  28.068711 -82.437068  \n",
      "3      NaN  27.534335 -82.506753  \n",
      "4      NaN  25.728644 -80.242038  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2017.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20171.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargar el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadir la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizar las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2017.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos columnas innesesarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                  direccion  \\\n",
      "0           Titusville Mall                  Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center     North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza  East Fletcher Avenue, Hillsborough County   \n",
      "3            Manatee County                             Florida, 34222   \n",
      "4                  CocoWalk                         3015, Grand Avenue   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3       Manatee County  Florida  United States         34222 -82.506753   \n",
      "4    Miami-Dade County  Florida  United States         33133 -80.242038   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.534335  \n",
      "4  25.728644  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2017.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2017.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            direccion        ciudad\n",
      "0           Narvaez Drive, Titusville       Brevard\n",
      "1     North 20th Street, Hillsborough  Hillsborough\n",
      "2  East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3                      Florida, 34222       Manatee\n",
      "4                  3015, Grand Avenue    Miami-Dade\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2017.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos Id nombre e id Ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2017.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2017_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el area de ML se combierte el archivo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2017_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2017_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2017_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (348, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           156\n",
      "addr:street    247\n",
      "addr:city      254\n",
      "addr:state     269\n",
      "amenity        348\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  348.000000  348.000000\n",
      "mean       NaN   27.609325  -81.547966\n",
      "std        NaN    1.398079    1.328651\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.254266  -82.176331\n",
      "50%        NaN   27.906197  -81.459963\n",
      "75%        NaN   28.467118  -80.337658\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           187\n",
      "addr:street     88\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           348\n",
      "@lon           348\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos direciones en base a la latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3                      None   \n",
      "4                  CocoWalk   \n",
      "\n",
      "                                         addr:street   addr:city addr:state  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...  Titusville        NaN   \n",
      "1  Landmark Shopping Center, North 20th Street, H...        None        NaN   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...        None        NaN   \n",
      "3      Manatee County, Florida, 34222, United States        None        NaN   \n",
      "4  CocoWalk, 3015, Grand Avenue, Dinner Key, Miam...       Miami        NaN   \n",
      "\n",
      "   amenity       @lat       @lon  \n",
      "0      NaN  28.575472 -80.802640  \n",
      "1      NaN  28.068730 -82.436298  \n",
      "2      NaN  28.068711 -82.437068  \n",
      "3      NaN  27.534335 -82.506753  \n",
      "4      NaN  25.728644 -80.242038  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2016.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremos los datos descargados en la columna direcciones , eliminamos columnas inesesarias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                  direccion  \\\n",
      "0           Titusville Mall                  Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center     North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza  East Fletcher Avenue, Hillsborough County   \n",
      "3            Manatee County                             Florida, 34222   \n",
      "4                  CocoWalk                         3015, Grand Avenue   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3       Manatee County  Florida  United States         34222 -82.506753   \n",
      "4    Miami-Dade County  Florida  United States         33133 -80.242038   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.534335  \n",
      "4  25.728644  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2016.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra County "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            direccion        ciudad\n",
      "0           Narvaez Drive, Titusville       Brevard\n",
      "1     North 20th Street, Hillsborough  Hillsborough\n",
      "2  East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3                      Florida, 34222       Manatee\n",
      "4                  3015, Grand Avenue    Miami-Dade\n"
     ]
    }
   ],
   "source": [
    "4\n",
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos id_nombre y Id_ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2016_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2016_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2016_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (348, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name              addr:street      addr:city  \\\n",
      "0           Titusville Mall                      NaN            NaN   \n",
      "1  Landmark Shopping Center                      NaN            NaN   \n",
      "2            LandMark Plaza                      NaN            NaN   \n",
      "3  Ellenton Premium Outlets  Factory Shops Boulevard       Ellenton   \n",
      "4                  CocoWalk             Grand Avenue  Coconut Grove   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.534335 -82.506753  \n",
      "4        NaN      NaN  25.728644 -80.242038  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           156\n",
      "addr:street    247\n",
      "addr:city      254\n",
      "addr:state     269\n",
      "amenity        348\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  348.000000  348.000000\n",
      "mean       NaN   27.609325  -81.547966\n",
      "std        NaN    1.398079    1.328651\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.254266  -82.176331\n",
      "50%        NaN   27.906197  -81.459963\n",
      "75%        NaN   28.467118  -80.337658\n",
      "max        NaN   30.520546  -80.037552\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           187\n",
      "addr:street     88\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           348\n",
      "@lon           348\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2016.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desacrgamos datos geograficos mediante latitud y longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2018.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20181.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2018.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos datos de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2018.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2018.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borramos la palabra County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2018.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos Id_nombre y id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "5\n",
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2018.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2018_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformamos a formato parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2018_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2018_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2018_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state  amenity  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN      NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN      NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN      NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN      NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN      NaN   \n",
      "\n",
      "        @lat       @lon  \n",
      "0  28.575472 -80.802640  \n",
      "1  28.068730 -82.436298  \n",
      "2  28.068711 -82.437068  \n",
      "3  27.747511 -82.680299  \n",
      "4  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           194\n",
      "addr:street    301\n",
      "addr:city      310\n",
      "addr:state     325\n",
      "amenity        404\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           205\n",
      "addr:street     89\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           404\n",
      "@lon           404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2019.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisamos  estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos datos faltantes en base ala longitud y latitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2019.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2019.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la imformacion de la coluna direcciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2019.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2019.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2019.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CReamos las columnas Id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2019.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2019_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2019_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2019_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2019_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state  amenity  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN      NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN      NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN      NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN      NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN      NaN   \n",
      "\n",
      "        @lat       @lon  \n",
      "0  28.575472 -80.802640  \n",
      "1  28.068730 -82.436298  \n",
      "2  28.068711 -82.437068  \n",
      "3  27.747511 -82.680299  \n",
      "4  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           194\n",
      "addr:street    301\n",
      "addr:city      310\n",
      "addr:state     325\n",
      "amenity        404\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           205\n",
      "addr:street     89\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           404\n",
      "@lon           404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2020.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos informacion faltante en base a la longuitud y latitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2020.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2020.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos nombres de columnas y eliminamos las menos relebantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "3\n",
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2020.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2020.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boramos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2020.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna Id nombre i Id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2020.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2020_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combertimos a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2020_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2020_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2020_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state  amenity  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN      NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN      NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN      NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN      NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN      NaN   \n",
      "\n",
      "        @lat       @lon  \n",
      "0  28.575472 -80.802640  \n",
      "1  28.068730 -82.436298  \n",
      "2  28.068711 -82.437068  \n",
      "3  27.747511 -82.680299  \n",
      "4  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           194\n",
      "addr:street    301\n",
      "addr:city      310\n",
      "addr:state     325\n",
      "amenity        404\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           205\n",
      "addr:street     89\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           404\n",
      "@lon           404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2021.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos los datos que nos faltan en base a longitud y latitud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2021.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2021.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la direccion de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2021.csv\"  \n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2021.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2021.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre y id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2021.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2021_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a formato parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2021_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2021_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2021_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state  amenity  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN      NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN      NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN      NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN      NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN      NaN   \n",
      "\n",
      "        @lat       @lon  \n",
      "0  28.575472 -80.802640  \n",
      "1  28.068730 -82.436298  \n",
      "2  28.068711 -82.437068  \n",
      "3  27.747511 -82.680299  \n",
      "4  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           194\n",
      "addr:street    301\n",
      "addr:city      310\n",
      "addr:state     325\n",
      "amenity        404\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           205\n",
      "addr:street     89\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           404\n",
      "@lon           404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2022.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos datos faltantes en base a la longitud y latitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2022.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2022.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se extrae los datos de la columna direcciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2022.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2022.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamps las columnas id nombres y id ciudad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2022.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2022_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato .parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2022_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2022_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2022_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state  amenity  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN      NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN      NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN      NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN      NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN      NaN   \n",
      "\n",
      "        @lat       @lon  \n",
      "0  28.575472 -80.802640  \n",
      "1  28.068730 -82.436298  \n",
      "2  28.068711 -82.437068  \n",
      "3  27.747511 -82.680299  \n",
      "4  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name           194\n",
      "addr:street    301\n",
      "addr:city      310\n",
      "addr:state     325\n",
      "amenity        404\n",
      "@lat             0\n",
      "@lon             0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity        float64\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           205\n",
      "addr:street     89\n",
      "addr:city       59\n",
      "addr:state       1\n",
      "amenity          0\n",
      "@lat           404\n",
      "@lon           404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2023.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos datos nesesarios basados en la longitud y latitud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state  amenity       @lat       @lon  \n",
      "0        NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2023.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2023.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos columnas poco relebantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2023.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre y id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "5\n",
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasasmos el archivo a formato parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2023_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2023_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parques de diverciones Centros comerciales 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (404, 8)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'addr:postcode', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                       name addr:street addr:city addr:state addr:postcode  \\\n",
      "0           Titusville Mall         NaN       NaN        NaN           NaN   \n",
      "1  Landmark Shopping Center         NaN       NaN        NaN           NaN   \n",
      "2            LandMark Plaza         NaN       NaN        NaN           NaN   \n",
      "3  Lakeview Shopping Center         NaN       NaN        NaN           NaN   \n",
      "4       Carrollwood Commons         NaN       NaN        NaN           NaN   \n",
      "\n",
      "   amenity       @lat       @lon  \n",
      "0      NaN  28.575472 -80.802640  \n",
      "1      NaN  28.068730 -82.436298  \n",
      "2      NaN  28.068711 -82.437068  \n",
      "3      NaN  27.747511 -82.680299  \n",
      "4      NaN  28.087608 -82.505597  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name             194\n",
      "addr:street      301\n",
      "addr:city        310\n",
      "addr:state       325\n",
      "addr:postcode    311\n",
      "amenity          404\n",
      "@lat               0\n",
      "@lon               0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name              object\n",
      "addr:street       object\n",
      "addr:city         object\n",
      "addr:state        object\n",
      "addr:postcode     object\n",
      "amenity          float64\n",
      "@lat             float64\n",
      "@lon             float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "       amenity        @lat        @lon\n",
      "count      0.0  404.000000  404.000000\n",
      "mean       NaN   27.641876  -81.533718\n",
      "std        NaN    1.430175    1.274343\n",
      "min        NaN   25.452593  -87.298363\n",
      "25%        NaN   26.246406  -82.138606\n",
      "50%        NaN   27.922614  -81.552508\n",
      "75%        NaN   28.471022  -80.339058\n",
      "max        NaN   30.520546  -80.035268\n",
      "\n",
      "Valores únicos por columna:\n",
      "name             205\n",
      "addr:street       89\n",
      "addr:city         59\n",
      "addr:state         1\n",
      "addr:postcode     82\n",
      "amenity            0\n",
      "@lat             404\n",
      "@lon             404\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2024.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los datos faltantes en base ala latitud y longitud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name  \\\n",
      "0           Titusville Mall   \n",
      "1  Landmark Shopping Center   \n",
      "2            LandMark Plaza   \n",
      "3  Lakeview Shopping Center   \n",
      "4       Carrollwood Commons   \n",
      "\n",
      "                                         addr:street         addr:city  \\\n",
      "0  Titusville Mall, Narvaez Drive, Titusville, Br...        Titusville   \n",
      "1  Landmark Shopping Center, North 20th Street, H...              None   \n",
      "2  LandMark Plaza, East Fletcher Avenue, Hillsbor...              None   \n",
      "3  Lakeview Shopping Center, Hartford Street Sout...  Saint Petersburg   \n",
      "4  Carrollwood Commons, North Dale Mabry Highway,...              None   \n",
      "\n",
      "  addr:state addr:postcode  amenity       @lat       @lon  \n",
      "0        NaN           NaN      NaN  28.575472 -80.802640  \n",
      "1        NaN           NaN      NaN  28.068730 -82.436298  \n",
      "2        NaN           NaN      NaN  28.068711 -82.437068  \n",
      "3        NaN           NaN      NaN  27.747511 -82.680299  \n",
      "4        NaN           NaN      NaN  28.087608 -82.505597  \n"
     ]
    }
   ],
   "source": [
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 2024.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques de diverciones Centros comerciales 20161.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicar la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardar los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2024.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "\n",
    "# Mostrar las primeras filas procesadas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos colunas innecesarias y cambiamos los nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "                     nombre                                      direccion  \\\n",
      "0           Titusville Mall                      Narvaez Drive, Titusville   \n",
      "1  Landmark Shopping Center         North 20th Street, Hillsborough County   \n",
      "2            LandMark Plaza      East Fletcher Avenue, Hillsborough County   \n",
      "3  Lakeview Shopping Center        Hartford Street South, Saint Petersburg   \n",
      "4       Carrollwood Commons  North Dale Mabry Highway, Hillsborough County   \n",
      "\n",
      "                ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0       Brevard County  Florida  United States         32780 -80.802640   \n",
      "1  Hillsborough County  Florida  United States         33613 -82.436298   \n",
      "2  Hillsborough County  Florida  United States         33613 -82.437068   \n",
      "3      Pinellas County  Florida  United States         33711 -82.680299   \n",
      "4  Hillsborough County  Florida  United States         33618 -82.505597   \n",
      "\n",
      "     latitud  \n",
      "0  28.575472  \n",
      "1  28.068730  \n",
      "2  28.068711  \n",
      "3  27.747511  \n",
      "4  28.087608  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/Parques_direcciones_actualizadas_2024.csv\" \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Saparamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 direccion        ciudad\n",
      "0                Narvaez Drive, Titusville       Brevard\n",
      "1          North 20th Street, Hillsborough  Hillsborough\n",
      "2       East Fletcher Avenue, Hillsborough  Hillsborough\n",
      "3  Hartford Street South, Saint Petersburg      Pinellas\n",
      "4   North Dale Mabry Highway, Hillsborough  Hillsborough\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2024.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las columnas id nombre y id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2024.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2024_final.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el codigo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2024_final.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/Parques_de_diversiones_Centros_comerciales_2024_final.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:\\Nueva carpeta\\overpass-turbo.eu\\Parques_de_diversiones_Centros_comerciales_2024_final.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
