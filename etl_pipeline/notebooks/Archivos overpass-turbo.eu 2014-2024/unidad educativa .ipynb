{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias\n",
    "import pandas as pd\n",
    "import numpy as ny\n",
    "import googlemaps\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderInsufficientPrivileges\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del dataset: (6182, 7)\n",
      "\n",
      "Nombres de las columnas: ['name', 'addr:street', 'addr:city', 'addr:state', 'amenity', '@lat', '@lon']\n",
      "\n",
      "Primeras filas del dataset:\n",
      "                name addr:street addr:city addr:state amenity       @lat  \\\n",
      "0   Allentown School         NaN       NaN        NaN  school  30.764356   \n",
      "1    Alliance School         NaN       NaN        NaN  school  30.621027   \n",
      "2     Allison School         NaN       NaN        NaN  school  30.339651   \n",
      "3  Alta Vista School         NaN       NaN        NaN  school  27.325324   \n",
      "4    Anderson School         NaN       NaN        NaN  school  30.034406   \n",
      "\n",
      "        @lon  \n",
      "0 -87.075245  \n",
      "1 -85.136313  \n",
      "2 -83.826548  \n",
      "3 -82.518152  \n",
      "4 -82.070106  \n",
      "\n",
      "Valores nulos por columna:\n",
      "name            327\n",
      "addr:street    4711\n",
      "addr:city      4850\n",
      "addr:state     4996\n",
      "amenity           0\n",
      "@lat              0\n",
      "@lon              0\n",
      "dtype: int64\n",
      "\n",
      "Tipos de datos:\n",
      "name            object\n",
      "addr:street     object\n",
      "addr:city       object\n",
      "addr:state      object\n",
      "amenity         object\n",
      "@lat           float64\n",
      "@lon           float64\n",
      "dtype: object\n",
      "\n",
      "Estadísticas descriptivas para columnas numéricas:\n",
      "              @lat         @lon\n",
      "count  6182.000000  6182.000000\n",
      "mean     26.627091   -81.158732\n",
      "std       8.501148     3.809886\n",
      "min     -34.414405   -87.568596\n",
      "25%      26.018405   -82.342810\n",
      "50%      27.876852   -81.410662\n",
      "75%      28.896499   -80.274274\n",
      "max      30.987127   -55.155209\n",
      "\n",
      "Valores únicos por columna:\n",
      "name           5340\n",
      "addr:street    1215\n",
      "addr:city       288\n",
      "addr:state        3\n",
      "amenity           3\n",
      "@lat           6083\n",
      "@lon           6049\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# 2. Realizamos una revisión general\n",
    "print(\"Dimensiones del dataset:\", data.shape)\n",
    "print(\"\\nNombres de las columnas:\", data.columns.tolist())\n",
    "print(\"\\nPrimeras filas del dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# 3. Mostramos la calidad de los datos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# 4. Realisaos una estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas para columnas numéricas:\")\n",
    "print(data.describe())\n",
    "\n",
    "print(\"\\nValores únicos por columna:\")\n",
    "print(data.nunique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                name                                        addr:street  \\\n",
      "0   Allentown School  Allentown School, 6180, Central School Road, M...   \n",
      "1    Alliance School  Alliance School, Alliance Road, Sink Creek, Ja...   \n",
      "2     Allison School  Allison School, Lanier Grade, Jefferson County...   \n",
      "3  Alta Vista School  Alta Vista School, South Euclid Avenue, Saraso...   \n",
      "4    Anderson School  Anderson School, Northeast 17th Avenue, Bradfo...   \n",
      "\n",
      "  addr:city addr:state amenity       @lat       @lon  \n",
      "0      None        NaN  school  30.764356 -87.075245  \n",
      "1      None        NaN  school  30.621027 -85.136313  \n",
      "2      None        NaN  school  30.339651 -83.826548  \n",
      "3  Sarasota        NaN  school  27.325324 -82.518152  \n",
      "4      None        NaN  school  30.034406 -82.070106  \n"
     ]
    }
   ],
   "source": [
    "# 1. Cargamos los datos\n",
    "file_path = r'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv'  \n",
    "data = pd.read_csv(file_path, delimiter='\\t')  \n",
    "\n",
    "# Configuramos el geolocalizador con un user_agent único\n",
    "geolocator = Nominatim(user_agent=\"mi_proyecto_geocodificacion_v1\")\n",
    "\n",
    "# Función para realizar la geocodificación inversa y buscar el nombre del lugar\n",
    "def reverse_geocode(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), timeout=10)\n",
    "        if location:\n",
    "            address = location.address\n",
    "            city = location.raw.get('address', {}).get('city', None)\n",
    "            name = location.raw.get('name', None) or location.raw.get('address', {}).get('attraction', None)\n",
    "            return address, city, name\n",
    "        else:\n",
    "            return \"Dirección no encontrada\", None, None\n",
    "    except GeocoderTimedOut:\n",
    "        return \"Tiempo de espera agotado\", None, None\n",
    "    except GeocoderInsufficientPrivileges:\n",
    "        return \"Bloqueado: Verificar API o User-Agent\", None, None\n",
    "\n",
    "# Limpieza inicial del archivo\n",
    "input_file = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"\n",
    "cleaned_file = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 201412.csv\"\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, open(cleaned_file, 'w', encoding='utf-8') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():  # Omitir líneas vacías\n",
    "            outfile.write(line)\n",
    "\n",
    "# Cargamos el archivo limpio\n",
    "try:\n",
    "    data = pd.read_csv(cleaned_file, sep='\\t', encoding='utf-8')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")\n",
    "    data = pd.read_csv(cleaned_file, sep=',', encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "# Añadimos  la columna 'name' si no existe\n",
    "if 'name' not in data.columns:\n",
    "    data['name'] = None\n",
    "\n",
    "# Actualizamos las columnas addr:street, addr:city y name\n",
    "def update_address(row):\n",
    "    if pd.notna(row['@lat']) and pd.notna(row['@lon']):\n",
    "        address, city, name = reverse_geocode(row['@lat'], row['@lon'])\n",
    "        row['addr:street'] = address\n",
    "        row['addr:city'] = city\n",
    "        row['name'] = name\n",
    "    return row\n",
    "\n",
    "# Aplicamos la función fila por fila con pausas para respetar los límites de solicitudes\n",
    "for index, row in data.iterrows():\n",
    "    data.loc[index] = update_address(row)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Guardamos los resultados\n",
    "output_file = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"\n",
    "data.to_csv(output_file, index=False)\n",
    "print(data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los datos de la columna direccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos imformacion de la columna nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                               direccion  \\\n",
      "0   Allentown School               6180, Central School Road   \n",
      "1    Alliance School               Alliance Road, Sink Creek   \n",
      "2     Allison School          Lanier Grade, Jefferson County   \n",
      "3  Alta Vista School           South Euclid Avenue, Sarasota   \n",
      "4    Anderson School  Northeast 17th Avenue, Bradford County   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2   Jefferson County  Florida  United States   No definido -83.826548   \n",
      "3    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "4    Bradford County  Florida  United States         32058 -82.070106   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  30.339651  \n",
      "3  27.325324  \n",
      "4  30.034406  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la columna county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         direccion      ciudad\n",
      "0        6180, Central School Road  Santa Rosa\n",
      "1        Alliance Road, Sink Creek     Jackson\n",
      "2          Lanier Grade, Jefferson   Jefferson\n",
      "3    South Euclid Avenue, Sarasota    Sarasota\n",
      "4  Northeast 17th Avenue, Bradford    Bradford\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamops la columna id nomnbre y id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformamos el archivo a .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos informacion de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                               direccion  \\\n",
      "0   Allentown School               6180, Central School Road   \n",
      "1    Alliance School               Alliance Road, Sink Creek   \n",
      "2     Allison School          Lanier Grade, Jefferson County   \n",
      "3  Alta Vista School           South Euclid Avenue, Sarasota   \n",
      "4    Anderson School  Northeast 17th Avenue, Bradford County   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2   Jefferson County  Florida  United States   No definido -83.826548   \n",
      "3    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "4    Bradford County  Florida  United States         32058 -82.070106   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  30.339651  \n",
      "3  27.325324  \n",
      "4  30.034406  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra countyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         direccion      ciudad\n",
      "0        6180, Central School Road  Santa Rosa\n",
      "1        Alliance Road, Sink Creek     Jackson\n",
      "2          Lanier Grade, Jefferson   Jefferson\n",
      "3    South Euclid Avenue, Sarasota    Sarasota\n",
      "4  Northeast 17th Avenue, Bradford    Bradford\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ccreamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2015.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                               direccion  \\\n",
      "0   Allentown School               6180, Central School Road   \n",
      "1    Alliance School               Alliance Road, Sink Creek   \n",
      "2     Allison School          Lanier Grade, Jefferson County   \n",
      "3  Alta Vista School           South Euclid Avenue, Sarasota   \n",
      "4    Anderson School  Northeast 17th Avenue, Bradford County   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2   Jefferson County  Florida  United States   No definido -83.826548   \n",
      "3    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "4    Bradford County  Florida  United States         32058 -82.070106   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  30.339651  \n",
      "3  27.325324  \n",
      "4  30.034406  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2016.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2016.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         direccion      ciudad\n",
      "0        6180, Central School Road  Santa Rosa\n",
      "1        Alliance Road, Sink Creek     Jackson\n",
      "2          Lanier Grade, Jefferson   Jefferson\n",
      "3    South Euclid Avenue, Sarasota    Sarasota\n",
      "4  Northeast 17th Avenue, Bradford    Bradford\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2016.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Creamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2016.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2016.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trasformamos el archivo a .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna direccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                               direccion  \\\n",
      "0   Allentown School               6180, Central School Road   \n",
      "1    Alliance School               Alliance Road, Sink Creek   \n",
      "2     Allison School          Lanier Grade, Jefferson County   \n",
      "3  Alta Vista School           South Euclid Avenue, Sarasota   \n",
      "4    Anderson School  Northeast 17th Avenue, Bradford County   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2   Jefferson County  Florida  United States   No definido -83.826548   \n",
      "3    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "4    Bradford County  Florida  United States         32058 -82.070106   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  30.339651  \n",
      "3  27.325324  \n",
      "4  30.034406  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         direccion      ciudad\n",
      "0        6180, Central School Road  Santa Rosa\n",
      "1        Alliance Road, Sink Creek     Jackson\n",
      "2          Lanier Grade, Jefferson   Jefferson\n",
      "3    South Euclid Avenue, Sarasota    Sarasota\n",
      "4  Northeast 17th Avenue, Bradford    Bradford\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                               direccion  \\\n",
      "0   Allentown School               6180, Central School Road   \n",
      "1    Alliance School               Alliance Road, Sink Creek   \n",
      "2     Allison School          Lanier Grade, Jefferson County   \n",
      "3  Alta Vista School           South Euclid Avenue, Sarasota   \n",
      "4    Anderson School  Northeast 17th Avenue, Bradford County   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2   Jefferson County  Florida  United States   No definido -83.826548   \n",
      "3    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "4    Bradford County  Florida  United States         32058 -82.070106   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  30.339651  \n",
      "3  27.325324  \n",
      "4  30.034406  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         direccion      ciudad\n",
      "0        6180, Central School Road  Santa Rosa\n",
      "1        Alliance Road, Sink Creek     Jackson\n",
      "2          Lanier Grade, Jefferson   Jefferson\n",
      "3    South Euclid Avenue, Sarasota    Sarasota\n",
      "4  Northeast 17th Avenue, Bradford    Bradford\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2017.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato .parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2018.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la imformacion de la coluna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           direccion      ciudad\n",
      "0                          6180, Central School Road  Santa Rosa\n",
      "1                          Alliance Road, Sink Creek     Jackson\n",
      "2                      South Euclid Avenue, Sarasota    Sarasota\n",
      "3                    Northeast 17th Avenue, Bradford    Bradford\n",
      "4  North Merritt Street, Warrington Village Apart...    Escambia\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las columnas id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2019.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           direccion      ciudad\n",
      "0                          6180, Central School Road  Santa Rosa\n",
      "1                          Alliance Road, Sink Creek     Jackson\n",
      "2                      South Euclid Avenue, Sarasota    Sarasota\n",
      "3                    Northeast 17th Avenue, Bradford    Bradford\n",
      "4  North Merritt Street, Warrington Village Apart...    Escambia\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos a punto parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2020.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna direccion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           direccion      ciudad\n",
      "0                          6180, Central School Road  Santa Rosa\n",
      "1                          Alliance Road, Sink Creek     Jackson\n",
      "2                      South Euclid Avenue, Sarasota    Sarasota\n",
      "3                    Northeast 17th Avenue, Bradford    Bradford\n",
      "4  North Merritt Street, Warrington Village Apart...    Escambia\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las columnas id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pasamos el archivo a formato .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2021.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2014.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos las columnas id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2022.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la informacion de la columna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           direccion      ciudad\n",
      "0                          6180, Central School Road  Santa Rosa\n",
      "1                          Alliance Road, Sink Creek     Jackson\n",
      "2                      South Euclid Avenue, Sarasota    Sarasota\n",
      "3                    Northeast 17th Avenue, Bradford    Bradford\n",
      "4  North Merritt Street, Warrington Village Apart...    Escambia\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2023.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unidad educativa 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos la imformacion de la coluna direccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis de datos:\n",
      "Valores vacíos por columna:\n",
      "nombre           0\n",
      "direccion        0\n",
      "ciudad           0\n",
      "estado           0\n",
      "pais             0\n",
      "codigo_postal    0\n",
      "longitud         0\n",
      "latitud          0\n",
      "dtype: int64\n",
      "Número de filas duplicadas: 0\n",
      "              nombre                                          direccion  \\\n",
      "0   Allentown School                          6180, Central School Road   \n",
      "1    Alliance School                          Alliance Road, Sink Creek   \n",
      "2  Alta Vista School                      South Euclid Avenue, Sarasota   \n",
      "3    Anderson School             Northeast 17th Avenue, Bradford County   \n",
      "4    Anderson School  North Merritt Street, Warrington Village Apart...   \n",
      "\n",
      "              ciudad   estado           pais codigo_postal   longitud  \\\n",
      "0  Santa Rosa County  Florida  United States         32570 -87.075245   \n",
      "1     Jackson County  Florida  United States         32448 -85.136313   \n",
      "2    Sarasota County  Florida  United States         34239 -82.518152   \n",
      "3    Bradford County  Florida  United States         32058 -82.070106   \n",
      "4    Escambia County  Florida  United States         32507 -87.281083   \n",
      "\n",
      "     latitud  \n",
      "0  30.764356  \n",
      "1  30.621027  \n",
      "2  27.325324  \n",
      "3  30.034406  \n",
      "4  30.389921  \n"
     ]
    }
   ],
   "source": [
    "# Cargamos  los datos \n",
    "archivo = \"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv\"  \n",
    "data = pd.read_csv(archivo)\n",
    "\n",
    "# Renombramos  las columnas\n",
    "data.rename(columns={\n",
    "    'addr:street': 'direccion',\n",
    "    'amenity': 'tipo',\n",
    "    '@lat': 'latitud',\n",
    "    '@lon': 'longitud'\n",
    "}, inplace=True)\n",
    "\n",
    "# Dividimos la columna direccion para extraer los datos\n",
    "direccion_split = data['direccion'].fillna('').str.split(',', expand=True)\n",
    "\n",
    "# Creamos las nuebas colunas a usar\n",
    "data['nombre'] = direccion_split[0].str.strip()\n",
    "\n",
    "# Extraemos la direccion combinando todas las partes de la dirección hasta el estado\n",
    "data['direccion'] = direccion_split.apply(\n",
    "    lambda row: ', '.join(filter(None, [row[1].strip(), row[2].strip()])), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos la ciudad siempre y cuando tenga la palabra County \n",
    "data['ciudad'] = direccion_split.apply(\n",
    "    lambda row: next((x.strip() for x in row if 'County' in str(x)), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Extraemos el estado  \"Florida\"\n",
    "data['estado'] = direccion_split.apply(lambda row: 'Florida', axis=1)\n",
    "\n",
    "# Extraemos  y validamos los códigos postales \n",
    "data['codigo_postal'] = direccion_split.apply(\n",
    "    lambda row: next((re.search(r'\\b\\d{5}\\b', str(x)).group(0) for x in row if re.search(r'\\b\\d{5}\\b', str(x))), 'No definido'), axis=1\n",
    ")\n",
    "\n",
    "# Separamos el peis \"United States\"\n",
    "data['pais'] = direccion_split.apply(lambda row: 'United States', axis=1)\n",
    "\n",
    "# Reordenamos las columnas \n",
    "data = data[['nombre', 'direccion', 'ciudad', 'estado', 'pais', 'codigo_postal', 'longitud', 'latitud']]\n",
    "\n",
    "# Eliminamos  duplicados\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Guardamos el resultado\n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv'\n",
    "data.to_csv(processed_file_path, index=False)\n",
    "\n",
    "# Análisamos de datos vacíos y duplicados\n",
    "vacios = data.isnull().sum()\n",
    "duplicados = data.duplicated().sum()\n",
    "\n",
    "print(\"Análisis de datos:\")\n",
    "print(\"Valores vacíos por columna:\")\n",
    "print(vacios)\n",
    "print(f\"Número de filas duplicadas: {duplicados}\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos la palabra county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la palabra 'County' de las columnas 'direccion' y 'ciudad'\n",
    "data['direccion'] = data['direccion'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "data['ciudad'] = data['ciudad'].str.replace(r'\\bCounty\\b', '', regex=True).str.strip()\n",
    "print(data[['direccion', 'ciudad']].head())\n",
    "\n",
    "# Guardamos el resultado \n",
    "processed_file_path = 'H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv'\n",
    "data.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la columna id nombre e id ciudad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Cargamos las rutas de los archivos\n",
    "input_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv\"\n",
    "output_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv\"\n",
    "nombres_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/nombres_unidades_educativas_dim.csv\"\n",
    "ciudades_dim_path = \"H:/Nueva carpeta/overpass-turbo.eu/ciudades_dim.csv\"\n",
    "\n",
    "# Cargamos los catálogos\n",
    "def load_catalog(path, key, value):\n",
    "    catalog = {}\n",
    "    if os.path.exists(path):\n",
    "        with open(path, mode='r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                catalog[row[key]] = int(row[value])\n",
    "    return catalog\n",
    "\n",
    "# Guardamos los catálogos de ciudad y nombres\n",
    "def save_catalog(path, catalog, key, value):\n",
    "    with open(path, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[value, key])\n",
    "        writer.writeheader()\n",
    "        for k, v in catalog.items():\n",
    "            writer.writerow({value: v, key: k})\n",
    "\n",
    "# Generamos IDs únicos\n",
    "def get_id(value, catalog):\n",
    "    if value not in catalog:\n",
    "        catalog[value] = len(catalog) + 1\n",
    "    return catalog[value]\n",
    "\n",
    "# Cargamos catálogos existentes\n",
    "nombres_dict = load_catalog(nombres_dim_path, \"nombre\", \"id_nombre\")\n",
    "ciudades_dict = load_catalog(ciudades_dim_path, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "# Procesamos archivo de entrada\n",
    "def process_file(input_path, output_path):\n",
    "    if not os.path.exists(input_path):\n",
    "        raise FileNotFoundError(f\"El archivo de entrada no existe: {input_path}\")\n",
    "\n",
    "    with open(input_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        original_fieldnames = reader.fieldnames\n",
    "\n",
    "        # Validamos las columnas columnas requeridas\n",
    "        required_cols = {\"nombre\", \"direccion\", \"latitud\", \"longitud\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"}\n",
    "        missing_cols = required_cols - set(original_fieldnames)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"El CSV no contiene las siguientes columnas requeridas: {', '.join(missing_cols)}\")\n",
    "\n",
    "        # Nombres de las columnas\n",
    "        fieldnames = [\n",
    "            \"id_nombre\", \"nombre\", \"direccion\", \"latitud\", \"longitud\",\n",
    "            \"id_ciudad\", \"ciudad\", \"estado\", \"pais\", \"codigo_postal\"\n",
    "        ]\n",
    "\n",
    "        # ordenamos y creamos el archivo de salida\n",
    "        with open(output_path, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "            for row in reader:\n",
    "                id_nombre = get_id(row[\"nombre\"], nombres_dict)\n",
    "                id_ciudad = get_id(row[\"ciudad\"], ciudades_dict)\n",
    "\n",
    "                new_row = {\n",
    "                    \"id_nombre\": id_nombre,\n",
    "                    \"nombre\": row[\"nombre\"],\n",
    "                    \"direccion\": row[\"direccion\"],\n",
    "                    \"latitud\": row[\"latitud\"],\n",
    "                    \"longitud\": row[\"longitud\"],\n",
    "                    \"id_ciudad\": id_ciudad,\n",
    "                    \"ciudad\": row[\"ciudad\"],\n",
    "                    \"estado\": row[\"estado\"],\n",
    "                    \"pais\": row[\"pais\"],\n",
    "                    \"codigo_postal\": row[\"codigo_postal\"]\n",
    "                }\n",
    "                writer.writerow(new_row)\n",
    "\n",
    "process_file(input_path, output_path)\n",
    "\n",
    "# Guardamos el catálogos actualizados \n",
    "save_catalog(nombres_dim_path, nombres_dict, \"nombre\", \"id_nombre\")\n",
    "save_catalog(ciudades_dim_path, ciudades_dict, \"ciudad\", \"id_ciudad\")\n",
    "\n",
    "print(\"Proceso completado con éxito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos el archivo a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido exitosamente a Parquet: H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el archivo CSV \n",
    "csv_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.csv\"\n",
    "\n",
    "# Ruta del archivo Parquet de salida\n",
    "parquet_path = r\"H:/Nueva carpeta/overpass-turbo.eu/unidad educativa/unidad educativa 2024.parquet\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Guardamos el archivo Parquet\n",
    "df.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"Archivo convertido exitosamente a Parquet: {parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
