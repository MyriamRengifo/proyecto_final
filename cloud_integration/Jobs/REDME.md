#  Jobs - Scripts y Procesos de Trabajo en Databricks

##  Descripci贸n

Este directorio contiene los notebooks utilizados para ejecutar los procesos de trabajo (**jobs**) en **Databricks**, organizados de manera secuencial. Estos notebooks est谩n dise帽ados para automatizar y estructurar tareas clave en el flujo de datos, desde la configuraci贸n inicial hasta la generaci贸n de informes.

---

##  Archivos en la Carpeta

1. **`1.Configuraci贸n del ETL.ipynb`**
   - **Prop贸sito:** Configura el entorno necesario para el pipeline ETL.
   - **Funciones Principales:**
     - Carga inicial de datos desde **DBFS**.
     - Configuraci贸n de par谩metros para el flujo de datos.

2. **`2.Cargar tablas delta.ipynb`**
   - **Prop贸sito:** Procesa y almacena los datos en formato **Delta Table** dentro de Databricks.
   - **Funciones Principales:**
     - Conversi贸n de datos procesados a **Delta Table**.
     - Configuraci贸n de particiones para optimizar consultas.

3. **`3.limpiar los archivos de ingreso.ipynb`**
   - **Prop贸sito:** Elimina o transforma registros err贸neos en los archivos cargados.
   - **Funciones Principales:**
     - Validaci贸n de la calidad de los datos.
     - Limpieza de datos duplicados o inconsistentes.

4. **`4.creaci贸n de informe con todo lo hecho anteriormente.ipynb`**
   - **Prop贸sito:** Resume y genera informes visuales de los procesos realizados anteriormente.
   - **Funciones Principales:**
     - Resumen estad铆stico del pipeline.
     - Generaci贸n de gr谩ficos y tablas clave para reportes.

5. **`conteo de datos.ipynb`**
   - **Prop贸sito:** Realiza el conteo de filas en el archivo `servicios_restaurantes` antes y despu茅s de la descarga.
   - **Funciones Principales:**
     - Comparaci贸n de la cantidad de datos procesados.
     - Validaci贸n del flujo de datos y detecci贸n de posibles p茅rdidas.

---

##  C贸mo Usar los Notebooks

1. **Secuencia de Ejecuci贸n**
   - Sigue el orden num茅rico para garantizar que los procesos se ejecuten correctamente:
     - Comienza con `1.Configuraci贸n del ETL.ipynb`.
     - Finaliza con `4.creaci贸n de informe con todo lo hecho anteriormente.ipynb`.

2. **Requisitos Previos**
   - Los datos deben estar cargados en **DBFS** antes de iniciar los procesos.
   - Aseg煤rate de tener acceso a los recursos y permisos necesarios en Databricks.

3. **Ejecuci贸n**
   - Crea los trabajos en la secci贸n de **Jobs** de Databricks utilizando los notebooks de manera secuencial.
   - Programa los triggers para que la actualizaci贸n se realice autom谩ticamente en el d铆a y la hora indicados.

---

##  Notas Adicionales

- **Versionamiento:** Todos los notebooks est谩n preparados para ser integrados en flujos automatizados de trabajo.
- **Errores Comunes:**
  - Revisa que las rutas de los archivos en **DBFS** est茅n correctamente configuradas en el notebook `1.Configuraci贸n del ETL.ipynb`.
  - Aseg煤rate de que los datos de entrada coincidan con las especificaciones de cada notebook.

---

Este archivo README proporciona una gu铆a clara y estructurada para comprender y ejecutar los procesos almacenados en esta carpeta. Ideal para documentar y mantener un flujo de trabajo organizado en Databricks. 
